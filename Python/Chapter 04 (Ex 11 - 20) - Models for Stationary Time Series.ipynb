{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.11**.  For the ARMA(1, 2) model $Y_t = 0.8 Y_{t - 1} + e_t + 0.7 e_{t-1} + 0.6 e_{t-2}$, show that\n",
    "\n",
    "**(a)** $\\rho_k = 0.8 \\rho_{k - 1}$ for $k > 2$.\n",
    "\n",
    "**(b)** $\\rho_2 = 0.8 \\rho_1 + 0.6 \\sigma_e^2 / \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For $k > 2$,\n",
    "\n",
    "$$ \\gamma_k = \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[0.8 Y_{t-1} + e_t + 0.7 e_{t-1} + 0.6 e_{t-2}, Y_{t - k}] = 0.8 \\text{Cov}[Y_{t - 1}, Y_{t - k}] = 0.8 \\gamma_{k - 1} $$\n",
    "\n",
    "Where we used the fact that $Y_{t - k}$ is independent from the later error variables $e_{t-2}, e_{t-1}, e_t$.  Therefore,  $\\gamma_k = 0.8 \\gamma_{k-1}$, and dividing by $\\gamma_0$ we get the result \n",
    "\n",
    "$$\\rho_k = 0.8 \\rho_{k-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** For $k = 2$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\gamma_2 &= \\text{Cov}[Y_t, Y_{t - 2}]  \\\\\n",
    "&= \\text{Cov}[0.8 Y_{t - 1} + e_t + 0.7 e_{t - 1} + 0.6 e_{t - 2}, Y_{t - 2}] \\\\\n",
    "&= \\text{Cov}[0.8 Y_{t - 1}, Y_{t - 2}] + \\text{Cov}[0.6 e_{t - 2}, Y_{t - 2}] \\\\\n",
    "&= 0.8 \\gamma_1 + 0.6 \\sigma_e^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, dividing by $\\gamma_0$, we get the result\n",
    "\n",
    "$$ \\rho_2 = 0.8 \\rho_1 + 0.6 \\frac{\\sigma_e^2}{\\gamma_0} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.12**.  Consider two MA(2) processes, one with $\\theta_1 = \\theta_2 = 1/6$ and another with $\\theta_1 = -1$ and $\\theta_2 = 6$.\n",
    "\n",
    "**(a)** Show that these processes have the same autocorrelation function.\n",
    "\n",
    "**(b)** How do the roots of the corresponding characteristic polynomials compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For any MA(2) process,\n",
    "\n",
    "$$ \\rho_1 = \\frac{-\\theta_1 + \\theta_1 \\theta_2}{1 + \\theta_1^2 + \\theta_2^2}\n",
    "\\quad \\text{and} \\quad\n",
    "\\rho_2 = \\frac{-\\theta_2}{1 + \\theta_1^2 + \\theta_2^2} $$\n",
    "\n",
    "and $\\rho_k = 0$ for $k > 2$.\n",
    "\n",
    "For the process with $\\theta_1 = \\theta_2 = 1/6$,\n",
    "\n",
    "$$ \\rho_1 = \\frac{-\\frac{1}{6} +  \\left(\\frac{1}{6}\\right)^2}{1 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{6}\\right)^2} = \\frac{-6 + 1}{36 + 1 + 1} = \\frac{-5}{38} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\rho_2 = \\frac{-\\frac{1}{6}}{1 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{6}\\right)^2} = \\frac{-6}{36 + 1 + 1} = -\\frac{3}{19} $$\n",
    "\n",
    "For the process with $\\theta_1 = -1$ and $\\theta_2 = 6$,\n",
    "\n",
    "$$ \\rho_1 = \\frac{-1 + 1 \\cdot 6}{1 + 1^2 + 6^2} = \\frac{5}{38} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\rho_2 = \\frac{-6}{1 + 1^2 + 6^2} = \\frac{3}{19} $$\n",
    "\n",
    "Therefore, both processes have the same autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The MA(2) characteristic equation is\n",
    "\n",
    "$$ 1 - \\theta_1 x - \\theta_2 x^2 = 0 $$\n",
    "\n",
    "with roots (potentially complex)\n",
    "\n",
    "$$ \\frac{-\\theta_1 \\pm \\sqrt{\\theta_1^2 + 4 \\theta_2}}{2 \\theta_2}$$\n",
    "\n",
    "For the process with $\\theta_1 = \\theta_2 = 1/6$, the roots are\n",
    "\n",
    "$$ \\frac{-\\frac{1}{6} \\pm \\sqrt{\\left( \\frac{1}{6} \\right)^2 + 4 \\frac{1}{6}} }{2 \\frac{1}{6}} = \\frac{-1 \\pm 5}{2} = \\{ -3, 2 \\} $$\n",
    "\n",
    "and for the process with $\\theta_1 = -1$ and $\\theta_2 = 6$, the roots are\n",
    "\n",
    "$$ \\frac{-(-1) \\pm \\sqrt{(-1)^2 + 4 \\cdot 6}}{2 \\cdot 6} = \\frac{1 \\pm 5}{12} = \\left\\{ -\\frac{1}{3}, \\frac{1}{2} \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.13**.  Let $\\{Y_t\\}$ be a stationary process with $\\rho_k = 0$ for $k > 1$.  Show that we must have $|\\rho_1| \\leq \\frac{1}{2} $.  (Hint: Consider  $\\text{Var}[Y_{n+1} + Y_n + Y_{n-1} + \\cdots \\pm Y_1]$ and then $\\text{Var}[Y_{n+1} - Y_n + Y_{n-1} - \\cdots \\pm Y_1]$.  Use the fact that both of these must be nonnegative for all $n$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{Var}\\left[ \\sum_{k=0}^t Y_{t - k} \\right] = \\sum_{i=0}^t \\sum_{j=0}^t \\text{Cov}[Y_{t - i}, Y_{t - j}] = t \\gamma_0 + 2(t - 1) \\gamma_1 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\text{Var}\\left[ \\sum_{k=0}^t (-1)^k Y_{t - k} \\right] = \\sum_{i=0}^t \\sum_{j=0}^t (-1)^{i+j}\\text{Cov}[Y_{t - i}, Y_{t - j}] = t \\gamma_0 - 2(t - 1) \\gamma_1 $$\n",
    "\n",
    "since all terms $\\text{Cov}[Y_{t - i}, Y_{t - j}] = \\gamma_{|i - j|}$ are zero for $|i - j| > 1$, and there are $t$ terms with $|i - j| = 0$ and $2(t - 1)$ terms with $|i - j| = 1$.\n",
    "\n",
    "As hinted, the variances must be non-negative, so\n",
    "\n",
    "$$ \n",
    "t \\gamma_0 + 2(t - 1) \\gamma_1 \\geq 0 \n",
    "\\quad \\text{and} \\quad\n",
    "t \\gamma_0 - 2(t - 1) \\gamma_1 \\geq 0 \n",
    "$$\n",
    "\n",
    "Dividing by $ \\text{Var}[Y_t] = \\gamma_0 \\geq 0$, we get\n",
    "\n",
    "$$ \n",
    "t + 2(t - 1) \\rho_1 \\geq 0 \n",
    "\\quad \\text{and} \\quad\n",
    "t - 2(t - 1) \\rho_1 \\geq 0 \n",
    "$$\n",
    "\n",
    "from where we bound $\\rho_1$ as\n",
    "\n",
    "$$ -\\frac{1}{2} \\frac{t}{t - 1} \\rho_1 \\leq \\frac{1}{2} \\frac{t}{t - 1}$$\n",
    "\n",
    "Since this is valid for all $t$, we can take the limits, obtaining\n",
    "\n",
    "$$ -\\frac{1}{2} \\leq \\rho_1 \\leq \\frac{1}{2} $$\n",
    "\n",
    "Proof:  if $\\rho_1 = \\frac{1}{2} + \\epsilon$, for some $\\epsilon > 0$, then we can pick sufficiently large $t$ to falsify $\\rho_1 \\leq \\frac{1}{2} \\frac{t}{t - 1}$, a contradiction.  An analogous argument applies for $\\rho_1 = -\\frac{1}{2} - \\epsilon$, for some $\\epsilon > 0$, and $ -\\frac{1}{2} \\frac{t}{t - 1} \\rho_1 $.\n",
    "\n",
    "Therefore, we have shown that\n",
    "\n",
    "$$ | \\rho_1 | < \\frac{1}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.14**.  Suppose that $\\{Y_t\\}$ is a zero mean, stationary process with $| \\rho_1 | < 0.5$ and $\\rho_k = 0$ for all $k > 1$.  Show that $\\{Y_t\\}$ must be representable as a MA(1) process.  That is, show that there is a white noise sequence $\\{e_t\\}$ such that $Y_t = e_t - \\theta e_{t - 1}$, where $\\rho_1$ is correct and $e_t$ is uncorrelated with $Y_{t - k}$ for $k > 0$.  (Hint:  Choose $\\theta$ such that $|\\theta| < 1$ and $\\rho_1 = -\\theta / (1 + \\theta^2)$; then let $e_t = \\sum_{j=0}^\\infty \\theta^j Y_{t - j}$.  If we assume that $\\{Y_t\\}$ is a normal process, $e_t$ will also be normal, and zero correlation is equivalent to independence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Note that the roots of $\\theta^2 \\rho_1 + \\theta + \\rho_1 = 0$ (assuming $\\rho_1 \\neq 0$) are\n",
    "\n",
    "$$ \\frac{-1 \\pm \\sqrt{1 - 4 \\rho_1}}{2 \\rho_1} $$\n",
    "\n",
    "which are real since $| \\rho_1 | < 0.5$.  Also note that $-1 + \\sqrt{1 - 4 \\rho_1} < |2 \\rho_1|$ and so the root\n",
    "\n",
    "$$ \\theta = \\frac{-1 + \\sqrt{1 - 4 \\rho_1}}{2 \\rho_1} $$\n",
    "\n",
    "satisfies $|\\theta| < 1$.  Choosing such $\\theta$, we get as hinted $\\rho_1 = -\\theta / (1 + \\theta^2)$ and $|\\theta| < 1$.\n",
    "\n",
    "Now, given the sequence $\\{Y_t\\}$ and this choice of $\\theta$, define the sequence $\\{e_t\\}$ by \n",
    "\n",
    "$$e_t = \\sum_{k=0}^t \\theta^k Y_{t - k}$$\n",
    "\n",
    "By construction, \n",
    "\n",
    "$$Y_t = e_t - \\theta e_{t - 1}$$\n",
    "\n",
    "The process $\\{e_t\\}$ has zero mean:\n",
    "\n",
    "$$ \\text{E}[e_t] = \\sum_{k=0}^t \\theta^k \\text{E}[Y_{t - k}] = \\sum_{k=0}^t \\theta^k \\cdot 0 = 0 $$\n",
    "\n",
    "For $k > 0$, the autocovariance is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[e_t, e_{t-k}] &= \\text{Cov}\\left[\\sum_{j=0}^t \\theta^j Y_{t-j}, \\sum_{j=0}^{t-k} \\theta^j Y_{t-j-k} \\right] \\\\ \n",
    "&= \\sum_{i=0}^t \\sum_{j=0}^{t-k} \\theta^{i+j} \\text{Cov}[Y_{t - i}, Y_{t - j - k}] \\\\\n",
    "&= \\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell + 1} \\gamma_0 + \\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell} \\gamma_1 + \\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell+2} \\gamma_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the first sum corresponds to the terms where $i = j = \\ell$, the second sum corresponds to the terms where $i = j + 1 = \\ell$, and the third sum correpsonds to the terms where $i = j + 2 = \\ell$; all other covariance terms involve $Y_a, Y_b$ with $|a - b| > 1$, and so $\\text{Cov}[Y_a, Y_b]$ is zero.\n",
    "\n",
    "We can now factor the geometric sums and use $\\gamma_1 = \\gamma_0 \\rho_1$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[e_t, e_{t-k}] \n",
    "&= \\theta \\gamma_0 \\left(\\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell} \\right) + \\gamma_0 \\rho_1 \\left(\\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell} \\right) + \\theta^2 \\gamma_0 \\rho_1 \\left( \\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell} \\right) \\\\\n",
    "&= \\left( \\sum_{\\ell=0}^{t-k+1} \\theta^{2\\ell} \\right) \\gamma_0 \\left( \\theta + \\rho_1 + \\theta^2 \\rho_1 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since selected $\\theta$ such that $\\theta + \\rho_1 + \\theta^2 \\rho_1 = 0$, the covariance above is 0.\n",
    "\n",
    "Therefore, $\\{e_t\\}$ has autocorrelation function $\\rho^{\\{e_t\\}}_k = 0$ for $k > 0$ and mean 0, and so it can be considered white noise -- and thus $\\{Y_t\\}$ can be represented as a MA(1) process, namely $Y_t = e_t - \\theta e_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.15**.  Consider the AR(1) model $Y_t = \\phi Y_{t-1} + e_t$.  Show that if $|\\phi| = 1$ the process cannot be stationary.  (Hint: Take variances on both sides.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Assuming the process is stationary, taking variance on both sides we get\n",
    "\n",
    "$$ \\gamma_0 = \\phi^2 \\gamma_0 + \\sigma_e^2 $$\n",
    "\n",
    "But if $|\\phi| = 1$, the above reduces to $0 = \\sigma_e^2$, a contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.16**.   Consider the \"nonstationary\" AR(1) model $Y_t = 3 Y_{t-1} + e_t$.\n",
    "\n",
    "**(a)** Show that $Y_t = -\\sum_{j=1}^\\infty \\left( \\frac{1}{3} \\right)^j e_{t + j}$ satisfies the AR(1) equation.\n",
    "\n",
    "**(b)** Show that the process defined in part (a) is stationary.\n",
    "\n",
    "**(c)** In what way is this solution unsatisfactory?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
