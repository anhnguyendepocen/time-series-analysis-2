{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamental Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**. Suppose $\\text{E}[X] = 2$, $\\text{Var}[X] = 9$, $\\text{E}[Y] = 0$, $\\text{Var}[Y] = 4$, and $\\text{Corr}(X, Y) = 0.25$.  Find:\n",
    "\n",
    "**(a)** $\\text{Var}[X + Y]$\n",
    "\n",
    "**(b)** $\\text{Cov}[X, X + Y]$\n",
    "\n",
    "**(c)** $\\text{Corr}[X + Y, X - Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Note that $\\text{Cov}[X, Y] = \\text{Corr}[X, Y] \\sqrt{\\text{Var}[X] \\text{Var}[Y]} = 0.25 \\cdot \\sqrt{9 \\cdot 4} = 1.5$.  Then,\n",
    "\n",
    "$$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Cov}[X, X + Y] = \\text{Cov}[X, X] + \\text{Cov}[X, Y] = \\text{Var}[X] + \\text{Cov}[X, Y] = 9 + 1.5 = 10.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "Computing the covariance:\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 9 - 4 = 5 $$\n",
    "\n",
    "Computing the variance of $X + Y$:\n",
    "\n",
    "$$ \\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5 $$\n",
    "\n",
    "Computing the variance of $X - Y$:\n",
    "\n",
    "$$ \\text{Var}[X - Y] = \\text{Var}[X] + \\text{Var}[Y] - \\text{Cov}[X, Y] = 9 + 4 - 1.5 = 11.5 $$\n",
    "\n",
    "So,\n",
    "\n",
    "$$ \\text{Corr}[X + Y, X - Y] = \\frac{\\text{Cov}[X + Y, X - Y]}{\\sqrt{\\text{Var}[X + Y] \\text{Var}[X - Y]}} = \\frac{5}{\\sqrt{14.5 \\cdot 11.5}} \\approx 0.3872 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**.  If $X$ and $Y$ are dependent but $\\text{Var}[X] = \\text{Var}[Y]$, find $\\text{Cov}[X + Y, X - Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**.  Let $X$ have a distribution with mean $\\mu$ and variance $\\sigma^2$, and let $Y_t = X$ for all $t$.\n",
    "\n",
    "**(a)** Show that $\\{ Y_t \\}$ is strictly and weakly stationary.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Sketch a \"typical\" time plot of $\\{ Y_t \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By definition, the distribution of $Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}$ is $n$ identical copies of a sample drawn from $X$, which is the same as the distribution of $Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}$ for any $k$, therefore $\\{Y_t\\}$ is strictly stationary.\n",
    "\n",
    "The mean function is $\\mu$ for all $t$, so it is constant over time, and the autocovariance function is $\\gamma_{t, t - k} = \\text{Cov}[Y_t, Y_{t - k}] = \\text{Var}[X] = \\sigma^2$ is also constant for all time $t$ and lag $k$, and so $\\{Y_t\\}$ is weakly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a), the covariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\text{Cov}[Y_t, Y_s] = \\text{Var}[X] = \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  A \"typical\" time plot of $\\{ Y_t \\}$ is a constant series, with all values equal to some value drawn from the distribution of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 3 * np.ones(10)\n",
    "nn = np.arange(1, 11)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(nn, X, marker='o', markerfacecolor='none', linestyle='solid', ms=5, markeredgecolor='blue')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'$Y_t$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4**.  Let $\\{ e_t \\}$ be a zero mean white noise process.  Suppose that the observed process is $Y_t = e_t + \\theta e_{t - 1}$, where $\\theta$ is either 3 or 1/3.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $\\{ Y_t \\}$ both when $\\theta = 3$ and when $\\theta = 1/3$.\n",
    "\n",
    "**(b)** You should have discovered that the time series is stationary regardless of the value of $\\theta$ and that the autocorrelation functions are the same for $\\theta = 3$ and $\\theta = 1/3$.  For simplicity, suppose that the process mean is known to be zero and the variance of $Y_t$ is known to be 1.  You observe the series $\\{Y_t\\}$ for $t = 1, 2, \\dots, n$ and suppose that you can produce good estimates of the autocorrelations $\\rho_k$.  Do you think that you could determine which value of $\\theta$ is correct (3 or 1/3) based on the estimate of $\\rho_k$?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The variance is:\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t + \\theta e_{t - 1}] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t - 1}] = (1 + \\theta^2) \\sigma_e^2 $$\n",
    "\n",
    "Also\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - 1} + \\theta e_{t - 2}] = \\theta \\text{Var}[e_{t-1}] = \\theta \\sigma_2^2\n",
    "$$\n",
    "\n",
    "and for element pairs with lag $k > 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - k} + \\theta e_{t - k - 1}] = 0 $$\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\begin{cases}\n",
    "(1 + \\theta^2) \\sigma_e^2 &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta \\sigma_e^2         &\\text{for } | t - s | = 1 \\\\\n",
    "0                         &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1                            &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta / (1 + \\theta^2)      &\\text{for } | t - s | = 1 \\\\\n",
    "0                            &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that $\\theta / (1 + \\theta^2) = 0.3$ for both $\\theta = 3$ and $\\theta = 1/3$ (this likely being the point of the exercise), so in either scenario the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1        &\\text{for } | t - s | = 0 \\\\\n",
    "0.3      &\\text{for } | t - s | = 1 \\\\\n",
    "0        &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a) and stated on the exercise, the series will have the same autocorrelation whether $\\theta = 3$ or $\\theta = 1/3$, so estimates of this property cannot be used to distinguish between the two candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5**.  Suppose $Y_t = 5 + 2t + X_t$, where $\\{ X_t \\}$ is zero-mean stationary series with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{ Y_t \\}$ stationary?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "\n",
    "$$ \\mu_t = \\text{E}[Y_t] = \\text{E}[5 + 2t + X_t] = 5 + 2t + \\text{E}[X_t] = 5 + 2t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The covariance for terms with lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[5 + 2t + X_t, 5 + 2(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is the same as the autocovariance function for $\\{X_t\\}$, $\\gamma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The series $\\{Y_t\\}$ is not stationary because the mean is not constant over time -- there is a time drift term, $2t$, in the mean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.6**.  Let $\\{ X_t \\}$ be a stationary time series, and let \n",
    "\n",
    "$$ Y_t = \\begin{cases}\n",
    "X_t &\\text{for } t \\text{ odd} \\\\\n",
    "X_t + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "**(a)**  Show that $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$ for all lags $k$.\n",
    "\n",
    "**(b)**  Is $\\{ Y_t \\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For every possible parity combination of $t$ and $k$ we have:\n",
    "\n",
    "- Even $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Even $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "\n",
    "Since $\\{X_t\\}$ is stationary, $\\text{Cov}[X_t, X_{t - k}]$ is free of $t$, and so $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No, as its mean function is not constant over time.  Given that $\\{ X_t \\}$ is stationary, it has a constant mean $\\overline{X}$, and we have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\begin{cases}\n",
    "\\overline{X} &\\text{for } t \\text{ odd} \\\\\n",
    "\\overline{X} + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "is not constant over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.7**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)**  Show that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary by finding the mean and the autocovariance function for $\\{ W_t \\}$.\n",
    "\n",
    "**(b)**  Show that $U_t = \\nabla^2 Y_t = \\nabla[Y_t - Y_{t - 1}] = Y_t - 2 Y_{t - 1} + Y_{t - 2}$ is stationary.  (You need not find the mean and autocovariance function for $\\{U_t\\}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Since $\\{Y_t\\}$ has a constant mean $\\overline{Y}$ over time, the mean of $W_t$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \\text{Var}[W_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\gamma_1 $$\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t,  Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "therefore the autocovariance function for $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since the autocovariance function is free of $k$ and the mean is constant over time, then $\\{W_t\\} is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the result of (a) to the series $\\{ W_t \\}$ rather than $ \\{ Y_t \\}$, we get that the series $\\{U_t\\} = \\{ \\nabla W_t \\}$ must also be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.8**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.  Show that for any positive integer $n$ and any constants $c_1, c_2, \\dots c_n$ the process $\\{ W_t \\}$ defined by $W_t = c_1 Y_t + c_2 Y_{t - 1} + \\cdots + c_n Y_{t - n + 1}$ is stationary.  (Note that Exercise 2.7 is a special case of this result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  If the mean of $\\{Y_t\\}$ is $\\overline{Y}$, then the mean of $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1} \\right] = \\sum_{i=1}^n c_i \\text{E}[Y_{t - i + 1}] = \\overline{Y} \\sum_{i=1}^n c_i $$\n",
    "\n",
    "which is constant over time.\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1}, \\sum_{j=1}^n c_i Y_{t - j + 1 - k}\\right] \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and since the difference in the indices of $\\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]$ does not depend on $t$, no matter which index is largest, then the overall expression is always a linear combination of the autocovariance function $\\gamma_m$ for $0 \\leq m \\leq n + k - 1$,\n",
    "\n",
    "$$ \\text{Cov}[W_t, W_{t - k}] = \\sum_{m=0}^{n + k - 1} d_{m, k} \\gamma_m $$\n",
    "\n",
    "for some constants $d_{m, k}$ that do not depend on $t$.  Therefore, $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.9**.  Suppose $Y_t = \\beta_0 + \\beta_1 t + X_t$, where $\\{X_t\\}$ is a zero-mean stationary series with autocovariance function $\\gamma_k$ and $\\beta_0$ and $\\beta_1$ are constants.\n",
    "\n",
    "**(a)**  Show that $\\{ Y_t \\}$ is not stationary but that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary.\n",
    "\n",
    "**(b)**  In general, show that if $Y_t = \\mu_t + X_t$, where $\\{ X_t \\}$ is a zero-mean stationary series and $\\mu_t$ is a polynomial in $t$ of degree $d$, then $\\nabla^m Y_t = \\nabla (\\nabla^{m - 1} Y_t)$ is stationary for $m \\geq d$ and nonstationary for $0 \\leq m \\leq d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\beta_0 + \\beta_1 t + X_t] = \\beta_0 + \\beta_1 t + \\overline{X} $$\n",
    "\n",
    "so the mean of $Y_t$ is not constant in time, and so $\\{Y_t\\}$ is not stationary.\n",
    "\n",
    "On the other hand, the mean of $\\{W_t\\}$ is constant in time,\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\beta_1 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Var}[W_t] &= \\text{Var}[Y_t - Y_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_1 + X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}{X_t} + \\text{Var}[X_{t - 1}] - 2 \\text{Cov}[X_t, X_{t - 1}] \\\\\n",
    "&= 2\\gamma_0 - 2\\gamma_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while the covariance of $W_t$ for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}, \n",
    "\\beta_0 + \\beta_1 (t - k) + X_{t - k} - \\beta_0 - \\beta_1 (t - k - 1) - X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_1 + X_t - X_{t - 1}, \\beta_1 + X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t - X_{t - 1}, X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t, X_{t - k}] - \\text{Cov}[X_t, X_{t - k - 1}] - \\text{Cov}[X_{t - 1}, X_{t - k - 1}] + \\text{Cov}[X_{t - 1}, X_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the autocovariance function for $\\{W_t\\}$ does not depend on $t$,\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since $\\{W_t\\}$ has a constant mean in time and an autocovariance function that does not depend on time, it is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  \n",
    "\n",
    "**Lemma 2.9.1**: if $\\{Y_t\\}$ is stationary, then $\\{ \\nabla Y_t \\}$ must also be stationary.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "- The mean of $\\{ \\nabla Y_t \\}$ constant (and zero):\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "- Assuming $\\{Y_t\\}$ has autocovariance function $\\gamma_k$, the variance of $\\{ \\nabla Y_t \\}$ is:\n",
    "\n",
    "  $$ \\text{Var}[ \\nabla Y_t ] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}{Y_t} + \\text{Var}[Y_{t - 1}] - 2 \\text{Cov}[Y_t, Y_{t - 1}] = 2\\gamma_0 - 2 \\gamma_1 $$\n",
    "\n",
    "  and the autocovariance for lag $k > 0$ is\n",
    "  \n",
    "  $$ \n",
    "  \\begin{align}\n",
    "  \\text{Cov}[\\nabla Y_t, \\nabla Y_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "  &= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t, Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "Therefore $\\{\\nabla Y_t\\}$ has a constant mean and an autocovariance function that does not depend on time, and so it is stationary.\n",
    "\n",
    "**Lemma 2.9.2**: If $\\mu_t$ is a polynomial of degree $d$, then $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1})$ is a polynomial of degree $d - 1$, for $1 \\leq k \\leq d$.\n",
    "\n",
    "**Proof**: Let $\\mu_t = \\sum_{j=0}^d c_j t^j$, for constants $c_j$.  \n",
    "\n",
    "- For $k = 1$, $\\nabla \\mu_t = \\mu_t - \\mu_{t - 1} = \\sum_{j=1}^d c_j (t^j - (t - 1)^j) $.  In each term, the coefficients of $t^j$ in $t^j$ and $(t - 1)^j$ cancel out, so $t^j - (t - 1)^j$ is a polynomial of degree $j - 1$, and the overall expression is a polynomial of degree $d - 1$.\n",
    "- For $k > 1$, $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1}) = \\nabla (a_t - a_{t - 1})$, where $a_t$ is a polynomial of degree $d - k + 1$ by induction, therefore $\\nabla^k \\mu_t$ is a polynomial of degree $d - k$.\n",
    "\n",
    "Now, we can prove the result from (b) by induction.  \n",
    "\n",
    "- Base case $d = 0$:  This is equivalent to Lemma 2.9.1.\n",
    "\n",
    "- Base case $d = 1$:  This is equivalent to the result in (a).\n",
    "\n",
    "- Induction step:\n",
    "  - If $m < d$, then $\\nabla_m Y_t$ is a polynomial $A$ in $t$ with degree $d - m$, plus a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$.  Therefore the expected value of $\\nabla_m Y_t$ is  $\\text{E}[A(t) - A(t - 1) + \\sum c_j X_j] = A(t) - A(t - 1) + \\overline{X} \\sum c_j$, which is a non-constant function of $t$.  Therefore the mean is not constant in time, and so $\\{ \\nabla^m Y_t \\}$ is not stationary.\n",
    "  - If $m > d$, then $\\nabla_m Y_t$ is a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times zeroes out the polynomial component).  Therefore, from Lemma 2.9.1, $\\{ \\nabla^m Y_t \\}$ is stationary.\n",
    "  - If $m = d$, then $\\nabla_m Y_t$ is a zero degree polynomial (a constant) plus a linear combination of of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times to bring the degree down to exactly 0).  Therefore, the expectation of $\\nabla^m Y_t$ is that constant plus $\\overline{X} \\sum c_j$, and so it does not change with time.  The zero-polynomial constant only scales up the covariance terms by a constant factor, and so the autocovariance also does not depend on the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.10**.  Let $\\{X_t\\}$ be a zero-mean, unit-variance stationary process with autocorrelation function $\\rho_k$.  Suppose that $\\mu_t$ is a nonconstant function and that $\\sigma_t$ is a positive-valued nonconstant function.  The observed series is formed as $Y_t = \\mu_t + \\sigma_t X_t$.\n",
    "\n",
    "**(a)**  Find the mean and the covariance function for the $\\{ Y_t \\}$ process.\n",
    "\n",
    "**(b)**  Show that the autocorrelation function for the $\\{ Y_t \\}$ process depends only on the time lag.  Is the $\\{Y_t\\}$ process stationary?\n",
    "\n",
    "**(c)**  Is it possible to have a time series with a constant mean and with $\\text{Corr}[Y_t, Y_{t - k}]$ free of $t$ but with $\\{Y_t\\}$ not stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\mu_t + \\sigma_t X_t] = \\mu_t + \\sigma_t \\text{E}[X_t] = \\mu_t $$\n",
    "\n",
    "and the covariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}[\\mu_t + \\sigma_t X_t, \\mu_s + \\sigma_s X_s] = \\sigma_t \\sigma_s \\text{Cov}[X_t, X_s] = \\sigma_t \\sigma_s \\rho_{t - s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The autocorrelation for the $\\{ Y_t \\}$ process is\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_t \\sigma_{t - k} \\rho_k}{\\sqrt{\\sigma_t^2 \\sigma_{t - k}^2}} = \\rho_k $$\n",
    "\n",
    "that is, it is the same as the autocorrelation for the $\\{X_t\\}$ process, and it depends only on the time lag.\n",
    "\n",
    "Note that this does not make the process stationary, both because it has nonconstant mean $\\mu_t$ and because stationarity requires time-independent autocovariance, not time-independent autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Yes -- for example, take $\\{X_t\\}$ as some series with more than two distinct non-zero autocorrelation values, make $\\mu_t = 0$ and $\\sigma_t = t$.  Then, even though $\\{Y_t\\}$ has constant mean 0, the autocovariance depends on the time,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\sigma_t \\sigma_{t - k} \\rho_k = t(t-k) \\rho_k$$\n",
    "\n",
    "despite that $\\text{Corr}[Y_t, Y_{t - k}] = \\rho_k$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.11**.  Suppose $\\text{Cov}[X_t, X_{t - k}] = \\gamma_k$ is free of $t$ but that $\\text{E}[X_t] = 3t$.\n",
    "\n",
    "**(a)** Is $\\{ X_t \\}$ stationary?\n",
    "\n",
    "**(b)** Let $Y_t = 7 - 3t + X_t$.  Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  No.  By definition, a series with a time-dependant mean is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  We have that $\\{ Y_t \\}$ has constant mean,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[7 - 3t + X_t] = 7 - 3t + \\text{E}[X_t] = 7 $$\n",
    "\n",
    "and its autocovariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[7 - 3t + X_t, 7 - 3(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "which is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.12**.  Suppose that $Y_t = e_t - e_{t - 12}$.  Show that $\\{Y_t\\}$ is stationary and that, for $k > 0$, its autocorrelation function is nonzero only for lag $k = 12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - e_{t - 12}] = \\text{E}[e_t] - \\text{E}[e_{t - 12}] = \\overline{e} - \\overline{e} = 0 $$\n",
    "\n",
    "so the series has constant mean.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - e_{t - 12}] = \\text{Var}[e_t] + \\text{Var}[e_{t - 12}] = 2 \\sigma_e^2 $$\n",
    "\n",
    "and for $k > 0$, its autocovariance function is \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - e_{t - 12}, e_{t - k} - e_{t - k - 12}] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\text{Cov}[e_{t - 12}, e_{t - k}] - \\text{Cov}[e_t, e{t - k - 12}] + \\text{Cov}[e_t, e_{t - k - 12}] \\\\\n",
    "&= -\\text{Cov}[e_{t - 12}, e_{t - k}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is zero if $k \\neq 12$ and $-\\sigma_e^2$ otherwise.\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "2 \\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "-\\sigma_e^2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "-1/2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the mean is constant and the autocovariance does not depend on the time, the series is stationary.  We have also shown that the only lag with a non-zero autocorrelation value is for $k = 12$, when the autocorrelation term is -1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.13**.  Let $Y_t = e_t - \\theta (e_{t - 1})^2$.  For this exercise, assume that the white noise series is normally distributed.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $Y_t$.\n",
    "\n",
    "**(b)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - \\theta (e_{t - 1})^2] = \\text{E}[e_t] - \\theta \\text{E}[e_{t-1}^2] = -\\theta \\sigma_e^2 $$\n",
    "\n",
    "The variance of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - \\theta (e_{t - 1})^2] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t-1}^2] = \\sigma_e^2 + \\theta^2 \\text{E}[e_{t-1}^4] = \\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 $$\n",
    "\n",
    "where we used the fact that the fourth moment of a normal distribution $N(0, \\sigma^2)$ is $\\text{E}[X^4] = 3 \\sigma^4$.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k > 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - k} - \\theta (e_{t - k - 1})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\theta \\text{Cov}[e_t, (e_{t - k - 1})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - k}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - k - 1})^2] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since all random variables within the covariances are independent.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k = 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - 1} - \\theta (e_{t - 2})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - 1}] - \\theta \\text{Cov}[e_t, (e_{t -2})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - 2})^2] \\\\\n",
    "&= - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] \\\\\n",
    "&= -\\theta \\left( \\text{E}[e_{t - 1}^3] - \\text{E}[e_{t - 1}^2] \\text{E}[e_{t - 1}]\\right) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the fact that the third moment of a normal distribution  $N(0, \\sigma^2)$ is $\\text{E}[X^3] = 0$ and the first moment of the distribution is its mean, which is also zero.\n",
    "\n",
    "Therefore, the autocovariance of $\\{Y_t\\}$ is\n",
    "\n",
    "$$\n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation is\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  The series has a constant mean $\\overline{Y} = \\theta \\sigma_e^2$, and an autocovariance which does not depend on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.14**.  Evaluate the mean and the covariance function for each of the following processes.  In each case, determine whether or not the process is stationary.\n",
    "\n",
    "**(a)**  $Y_t = \\theta_0 + t e_t$.\n",
    "\n",
    "**(b)**  $W_t = \\nabla Y_t$, where $Y_t$ is given in part (a).\n",
    "\n",
    "**(c)**  $Y_t = e_t e_{t - 1}$.  (You may assume that $\\{e_t\\}$ is normal white noise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\theta_0 + t e_t] = \\theta_0 + t \\text{E}[e_t] = \\theta_0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[\\theta_0 + t e_t] = t^2 \\text{Var}[e_t] = t^2 \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\theta_0 - \\theta_0 = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[\\nabla Y_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}[Y_t] + \\text{Var}[Y_{t - 1}] = t^2 \\sigma_e^2 + (t - 1)^2 \\sigma_e^2 = (2t^2 + 2t - 1) \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t e_{t-1}] = \\text{E}[e_t] \\text{E}[e_{t - 1}] = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t e_{t - 1}] = \\text{E}[e_t^2 e_{t - 1}^2] = \\text{E}[e_t^2] \\text{E}[ e_{t - 1}^2] = \\sigma_e^2 $$\n",
    "\n",
    "The autocorrelation of the process with lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{E}[Y_t Y_{t - k}] - \\text{E}[Y_t] \\text{E}[Y_{t - k}]  \\\\\n",
    "&= \\text{E}[e_t e_{t - 1} e_{t - k} e_{t - k - 1}] - 0 \\cdot 0 \\\\\n",
    "&= \\text{E}[e_t] \\text{E}[e_{t - 1} e_{t - k} e_{t - k - 1}] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance of the process is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the process has a constant mean and autocovariance that is free of time $t$, the process is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.15**.  Suppose that $X$ is a random variable with zero mean.  Define a time series by $Y_t = (-1)^t X$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the covariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[(-1)^t X] = (-1)^t \\text{E}[X] = (-1)^t \\cdot 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{E}[Y_t^2] - \\text{E}[Y_t]^2 = \\text{E}[(-1)^{2t}X^2]  - 0^2= \\text{E}[X^2] = \\text{Var}[X] = \\sigma_X^2 $$\n",
    "\n",
    "The covariance for even lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = \\sigma_X^2 $$\n",
    "\n",
    "and the covariance for odd lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, -Y_t] = -\\text{Var}[Y_t] = -\\sigma_X^2 $$\n",
    "\n",
    "Therefore the covariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_{t, s} = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } |t - s| \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } |t - s| \\text{ odd}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Yes.  The mean is constant, and the autocovariance function is $t$ free, depending only on the lag:\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } k \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } k \\text{ odd} \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.16**.  Suppose $Y_t = A + X_t$, where $\\{X_t\\}$ is stationary and $A$ is random but independent of $\\{ X_t \\}$.  Find the mean and covariance function for $\\{Y_t\\}$ in terms of the mean and autocovariance function for $\\{ X_t \\}$ and the mean and variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The mean is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[A + X_t] = \\text{E}[A] + \\text{E}[X_t] = \\mu_A + \\mu_X $$\n",
    "\n",
    "where $\\mu_A, \\mu_X$ are the means of $A$ and $\\{X_t\\}$.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[A + X_t] = \\text{Var}[A] + \\text{Var}[X_t] = \\sigma_A^2 + \\sigma_X^2 $$\n",
    "\n",
    "The covariance is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_s] &= \\text{Cov}[A + X_t, A + X_s]\n",
    "&= \\text{Cov}[A, A] + \\text{Cov}[A, X_s] + \\text{Cov}[X_t, A] + \\text{Cov}[X_t, X_s] \\\\\n",
    "&= \\sigma_A^2 + \\gamma_{t, s}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\sigma_A^2 + \\gamma_k $$\n",
    "\n",
    "where $\\gamma_k$ is the autocovariance function for $\\{X_t\\}$ and $\\sigma_A^2$ is the variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.17**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Let $\\overline{Y} = \\frac{1}{n} \\sum_{t=1}^n Y_t$.  Show that\n",
    "\n",
    "$$ \\text{Var}[\\overline{Y}] = \\frac{\\gamma_0}{n} + \\frac{2}{n} \\sum_{k=1}^{n -1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k \n",
    "= \\frac{1}{n} \\sum_{k=-n+1}^{n - 1} \\left(1 - \\frac{|k|}{n} \\right) \\gamma_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have\n",
    "\n",
    "$$ \n",
    "\\text{Var}[\\overline{Y}] = \\text{Var}\\left[ \\frac{1}{n} \\sum_{t=1}^n Y_i \\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\text{Cov}[Y_i, Y_j] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|j - i|}\n",
    "$$\n",
    "\n",
    "The term with lag $k = |j - i|$ appears $2(n - k)$ times, as that is the number of ways of selecting $i, j$ in the summation intervals to produce that lag.  The equalities in the result follow as a consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.18**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Define the sample variance as $ S^2 = \\frac{1}{n - 1} \\sum_{t=1}^n (Y_t - \\overline{Y})^2$.\n",
    "\n",
    "**(a)** First show that $\\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t = 1}^n (Y_t - \\overline{Y})^2 + n(\\overline{Y} - \\mu)^2$.\n",
    "\n",
    "**(b)** Use part (a) to show that\n",
    "\n",
    "**(c)**  $$ \\text{E}[S^2] = \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}] = \\gamma_0 - \\frac{2}{n - 1} \\sum_{k=1}^{n - 1} \\left( 1 - \\frac{n}{k} \\right) \\gamma_k $$\n",
    "\n",
    "(Use the results of Exercise 2.17 for the last expression.)\n",
    "\n",
    "**(d)** It $\\{Y_t \\}$ is a white noise process with variance $\\gamma_0$, show that $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 =  \\sum_{t=1}^n \\left((Y_t - \\overline{Y}) + (\\overline{Y} - \\mu)\\right)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + \\sum_{t=1}^n (\\overline{Y} - \\mu)^2 + 2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) $$\n",
    "\n",
    "But the last term is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) &= 2 (\\overline{Y} - \\mu) \\sum_{t=1}^n (Y_t - \\overline{Y}) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) \\left( \\left( \\sum_{t=1}^n Y_t \\right) - n \\overline{Y} \\right) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) ( n \\overline{Y} - n \\overline{Y} ) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the result follows,\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + n (\\overline{Y} - \\mu)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b, c)** We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[S^2] &= \\frac{1}{n - 1} \\text{E} \\left[ \\sum_{t=1}^n (Y_t - \\overline{Y})^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 - n (\\overline{Y} - \\mu)^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 \\right] - n (\\overline{Y} - \\mu)^2 \\right) \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{Var}[Y_t] - n \\text{Var}[\\overline{Y}] \\right) \\\\\n",
    "&= \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the second equality follows directly by replacing $\\text{Var}[\\overline{Y}]$ with the expression in the result of Exercise 2.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  If $\\{Y_t\\}$ is a white noise process with variance $\\gamma_0$, the autocovariance function is 0 for any positive lags.  Replacing these in the second expression for $\\text{E}[S^2]$ in the previous item, we get $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.19**.  Let $Y_1 = \\theta_0 + e_1$, and then for $t > 1$ define $Y_t$ recursively by $Y_t = \\theta_0 + Y_{t - 1} + e_t$.  Here $\\theta_0$ is a constant.  The process $\\{Y_t\\}$ is called a **random walk with drift**.\n",
    "\n",
    "**(a)** Show that $Y_t$ may be rewritten as $Y_t = t\\theta_0 + e_t + e_{t - 1} + \\cdots + e_1 $.\n",
    "\n",
    "**(b)** Find the mean function for $Y_t$.\n",
    "\n",
    "**(c)** Find the autocovariance function for $Y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The result is true for $t = 1$.  Assuming by induction it is valid for $t - 1$,\n",
    "\n",
    "$$ Y_t = \\theta_0 + Y_{t - 1} + e_t = \\theta_0 + (t - 1)\\theta_0 + \\sum_{i=1}^{t - 1} e_i + e_t = t \\theta_0 + \\sum_{i=1}^t e_i $$\n",
    "\n",
    "and so the result holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ t \\theta_0 + \\sum_{i=1}^t e_i \\right] = t \\theta_0 + \\sum_{i=1}^t \\text{E}[e_i] = t \\theta_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[t \\theta_0 + \\sum_{i=1}^t e_i\\right] = \\sum_{i=1}^t \\text{Var}[e_i] = t \\sigma_e^2 $$\n",
    "\n",
    "and the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[t\\theta_0 + \\sum_{i=1}^t e_i, (t - k)\\theta_0 + \\sum_{j=1}^{t - k} e_j \\right]  \\\\\n",
    "&= \\sum_{i=1}^t \\sum_{j=1}^{t - k} \\text{Cov}[e_i, e_j]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The covariance terms above are non-zero only for $i = j$, and so\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = (t - k) \\sigma_e^2 $$\n",
    "\n",
    "Therefore the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = t \\sigma_e^2 \\quad \\text{for } 1 \\leq t \\leq s $$\n",
    "\n",
    "This is exactly the same as the autocovariance function for the random walk, but now the mean is not constant -- it has a drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.20**.  Consider the standard random walk model where $Y_t = Y_{t - 1} + e_t$ with $Y_1 = e_t$.\n",
    "\n",
    "**(a)** Use the representation of $Y_t$ above to show that $\\mu_t = \\mu_{t - 1}$ for $t > 1$ with initial condition $\\mu_1 = \\text{E}[e_1] = 0$.  Hence show that $\\mu_t = 0$ for all $t$.\n",
    "\n",
    "**(b)** Similarly, show that $\\text{Var}[Y_t] = \\text{Var}[Y_{t - 1}] + \\sigma_e^2$ for $t > 1$ with $\\text{Var}[Y_1] = \\sigma-e^2$ and hence $\\text{Var}[Y_t] = t \\sigma_e^2$.\n",
    "\n",
    "**(c)** For $0 \\leq t \\leq s$, use $Y_s = Y_t + e_{t + 1} + e_{t + 2} + \\cdots + e_s$ to show that $\\text{Cov}[Y_t, Y_s] = \\text{Var}[Y_t]$ and, hence, that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By induction,\n",
    "\n",
    "$$\\mu_t = \\text{E}[Y_t] = \\text{E}[Y_{t - 1} + e_t] = \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = \\mu_{t - 1} + 0 = \\mu_{t - 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[Y_{t - 1} + e_t] = \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = (t - 1)\\sigma_e^2 + \\sigma_e^2 = t \\sigma_ e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] + 0 = t \\sigma_e^2$$\n",
    "\n",
    "For the case of $t > s$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.21**.  For a random walk with random starting value, let $Y_t = Y_0 + e_t + e_{t - 1} + \\cdots + e_1$ for $t > 0$, where $Y_0$ has a distribution with mean $\\mu_0$ and variance $\\sigma_0^2$.  Suppose further that $Y_0, e_1, \\dots, e_t$ are independent.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = \\mu_0$ for all $t$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = t \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(c)** Show that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(d)** Show that \n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} \\quad \\text{for } 0 \\leq t \\leq s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{E}[Y_0] + \\sum_{i=1}^t \\text{E}[e_i] = \\mu_0 + 0 = \\mu_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{Var}[Y_0] + \\sum_{i=1}^t \\text{Var}[e_i] = \\sigma_0^2 + t\\sigma_e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] = t\\sigma_e^2 + \\sigma_0^2 $$\n",
    "\n",
    "For the case where $s > t$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  Assuming $0 \\leq t \\leq s$, we have\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\frac{\\text{Cov}[Y_t, Y_s]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_s]}} = \n",
    "\\frac{t \\sigma_e^2 + \\sigma_0^2}{\\sqrt{(t \\sigma_e^2 + \\sigma_0^2)(s \\sigma_e^2 + \\sigma_0^2)}}\n",
    "= \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.22**.  Let $\\{e_t\\}$ be a zero-mean white noise process, and let $c$ be a constant with $|c| < 1$.  Define $Y_t$ recursively by $Y_t = c Y_{t - 1} + e_t$ with $Y_1 = e_1$.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = \\sigma_e^2 (1 + c^2 + c^4 + \\cdots + c^{2t - 2})$.  Is $\\{Y_t\\}$ stationary?\n",
    "\n",
    "**(c)** Show that\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-1}] = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "and, in general,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-k}] = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "Hint:  Argue that $Y_{t - 1}$ is independent of $e_t$.  Then, use\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[cY_{t - 1} + e_t, Y_{t - 1}] $$\n",
    "\n",
    "**(d)**  For large $t$, argue that\n",
    "\n",
    "$$ \\text{Var}[Y_t] \\approx \\frac{\\sigma_e^2}{1 - c^2} \n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Corr}[Y_t, Y_{t - k}] \\approx c^k\n",
    "\\quad \\text{for } k > 0 $$\n",
    "\n",
    "so that $\\{ Y_t \\}$ could be called **asymptotically stationary**.\n",
    "\n",
    "**(e)** Suppose now that we alter the initial condition and put $Y_1 = \\frac{e_1}{\\sqrt{1 - c^2}}$.  Show that now $\\{Y_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have $\\text{E}[Y_1] = \\text{E}[e_1] = 0$.  By induction,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[cY_{t - 1} + e_t] = c \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = c \\cdot 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  We have $\\text{Var}[Y_1] = \\text{Var}[e_1] = \\sigma_e^2$.  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[c Y_{t - 1} + e_t] = c^2 \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = c^2 \\left( \\sigma_e^2 \\sum_{j = 0}^{t - 2} c^{2j} \\right) + \\sigma_e^2 = \\sigma_e^2 \\sum_{j=0}^{t - 1} c^{2j} $$\n",
    "\n",
    "Since this depends on the value of $t$, the autocovariance is dependent on $t$ for lag 0, and so $\\{Y_t\\}$ is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have:\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - 1}] = c \\text{Var}[Y_{t - 1}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - 1}] = \\frac{ c \\text{Var}[Y_{t - 1}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - 1}]} } = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "In general\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k} = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = \\cdots = c^k \\text{Cov}[Y_{t - k}, Y_{t - k}] = c^k \\text{Var}[Y_{t - k}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{ c^k \\text{Var}[Y_{t - k}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]} } = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  For large $t$ -- or more precisely, on the limit as $t \\rightarrow \\infty$, the following geometric sum is\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\sum_{j = 0}^{t - 2} c^{2j} = \\frac{1}{1 - c^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Var}[Y_t] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Corr}[Y_t, Y_{t - k}] = \\lim_{t \\rightarrow \\infty} c^k \\sqrt{\\frac{Y_{t - k}}{Y_t}} = c^k $$\n",
    "\n",
    "These values at the limit do not depend on $t$, which aligns with the definition of $\\{Y_t\\}$ being asymptotically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** The mean of the new initial value is still 0, so the previous results on the mean being 0 still hold.\n",
    "\n",
    "The variance now becomes\n",
    "\n",
    "$$ \\text{Var}[Y_1] = \\frac{1}{1 - c^2} \\text{Var}[e_1] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and the autocovariance becomes, by induction,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = c \\left( \\frac{c^{k-1}}{1 - c^2} \\right) = \\frac{c^k}{1 - c^2} $$\n",
    "\n",
    "The conditions for stationarity are now satisfied, since the mean is still constant and the autocorrelation function is free of $t$, instead depending on the lag,\n",
    "\n",
    "$$ \\gamma_k = \\frac{c^k}{1 - c^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.23**. Two processes $\\{Z_t\\}$ and $\\{Y_t\\}$ are said to be **independent** if for any time points $t_1, t_2, \\dots, t_m$ and $s_1, s_2, \\dots, s_n$ the random variables $\\{Z_{t_1}, Z_{t_2}, \\dots, Z_{t_m}\\}$ are independent of the random variables $\\{ S_{s_1}, S_{s_2}, \\dots, S_{s_n}\\}$.  Show that if $\\{Z_t\\}$ and $\\{Y_t\\}$ are independent stationary processes, then $W_t = Z_t + Y_t$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Z_t] + \\text{E}[Y_t] = \\mu_Z + \\mu_Y $$\n",
    "\n",
    "and so the mean of $\\{W_t\\}$ does not depend on time.\n",
    "\n",
    "Let $\\alpha_k$ be the autocorrelation function of the $\\{Z_t\\}$ and $\\beta_k$ be the autocorrelation function of the $\\{Y_t\\}$.  We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Z_t + Y_t, Z_{t - k} + Y_{t - k}] \\\\\n",
    "&= \\text{Cov}[Z_t, Z_{t - k}] + \\text{Cov}[Z_t, Y_{t - k}] + \\text{Cov}[Z_{t - k}, Y_t] + \\text{Cov}[Y_t. Y_{t - k}] \\\\\n",
    "&= \\alpha_k + \\beta_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $\\text{Cov}[Z_t, Y_{t - k}] = \\text{Cov}[Z_{t - k}, Y_t]$ from the independence assumption.\n",
    "\n",
    "Therefore, the autocorrelation function of $\\{W_t\\}$ is free of $t$, and since the mean of $\\{W_t\\}$ is constant $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.24**.  Let $\\{X_t\\}$ be a time series in which we are interested.  However, because the measurement process itself is not perfect, we actually observe $Y_t = X_t + e_t$.  We assume that $\\{X_t\\}$ and $\\{e_t\\}$ are independent processes.  We call $X_t$ the **signal** and $e_t$ the **measurement noise** or **error process**.\n",
    "\n",
    "If $\\{X_t\\}$ is stationary with autocorrelation function $\\rho_k$, show that $\\{Y_t\\}$ is also stationary with\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} \\quad \\text{for } k \\geq 1 $$\n",
    "\n",
    "We call $\\sigma_X^2 / \\sigma_e^2$ the **signal-to-noise ratio**, or SNR.  Note that the larger the SNR, the closer the autocorrelation function of the observed process $\\{Y_t\\}$ is to the autocorrelation function of the desired signal $\\{X_t\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. The variance of the measured signal is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[X_t + e_t] = \\text{Var}[X_t] + \\text{Var}[e_t] = \\sigma_X^2 + \\sigma_e^2 $$\n",
    "\n",
    "We have, for $k \\geq 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + e_t, X_{t - k} + e_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\sigma_X^2 \\rho_k $$\n",
    "\n",
    "where we have used independence to cancel out the covariance between $X_a$ and $e_b$ and the white noise property to cancel out $\\text{Cov}[e_t, e_{t - k}]$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_X^2 \\rho_k}{\\sigma_X^2 + \\sigma_e^2} = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.25**.  Suppose $Y_t = \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] $, where $\\beta_0, f_1, f_2, \\dots, f_k$ are constants and $A_1, A_2, \\dots, A_k, B_1, B_2, \\dots, B_k$ are independent random variables with zero means and variances $\\text{Var}[A_i] = \\text{Var}[B_i] = \\sigma_i^2$. Show that $\\{Y_i\\}$ is stationary and find its covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have mean\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y_t] &= \\text{E}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\beta_0 + \\sum_{i=1}^k \\left( \\text{E}[A_i] \\cos(2 \\pi f_i t)  + \\text{E}[B_i] \\sin(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\beta_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[Y_t] &= \\text{Var}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\left (\\text{Var}[A_i] \\cos^2(2 \\pi f_i t) + \\text{Var}[B_i] \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos^2(2 \\pi f_i t) + \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the trigonometric identity $\\cos^2 \\alpha + \\sin^2 \\alpha = 1$.\n",
    "\n",
    "The autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)], \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i (t - k)) + B_i \\sin(2 \\pi f_i (t - k))] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\sum_{j=1}^k \\text{Cov}[A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t), A_j \\cos(2 \\pi f_i (t - k)) + B_j \\sin(2 \\pi f_i (t - k))] \\\\\n",
    "&= \\sum_{i=1}^k \\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) \\text{Var}[A_i] + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\text{Var}[B_i]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where, since the $A_i$'s and $B_i$'s are all independent, we cancel out all covariance terms that do not repeat the same random variable on both sides.\n",
    "\n",
    "Continuing the calculation, and using the trigonometric identify $\\cos (\\alpha - \\beta) = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}]| \n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i t - 2 \\pi f_i (t-k)) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is free of $t$.  The autocovariance function is, more explicitly,\n",
    "\n",
    "$$ \\gamma_k = \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k) $$\n",
    "\n",
    "and we have shown that $\\{Y_i\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.26**.  Define the function $\\Gamma_{t, s} = \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2]$.  In geostatistics, $\\Gamma_{t, s}$ is called the **semivariogram**.\n",
    "\n",
    "**(a)** Show that for a stationary process $\\Gamma_{t, s} = \\gamma_0 - \\gamma_{|t - s|}$.\n",
    "\n",
    "**(b)** A process is said to be **intrinsically stationary** if $\\Gamma_{t, s}$ depends only on the time difference $|t - s|$.  Show that the random walk process is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** For a stationary process,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] = \\frac{1}{2} \\text{E}[Y_t^2 + Y_s^2 - 2 Y_t Y_s] \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{E}[Y_t^2] + \\text{E}[Y_s^2] - 2 \\text{E}[Y_t Y_s] \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{Var}[Y_t] + \\text{E}[Y_t]^2 + \\text{Var}[Y_s] + \\text{E}[Y_s]^2 - 2 \\left( \\text{Cov}[Y_t, Y_s] + \\text{E}[Y_t] E[Y_s] \\right) \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\gamma_0 + \\overline{Y}^2 + \\gamma_0 + \\overline{Y}^2 - 2 (\\gamma_{|t - s|} + \\overline{Y}^2) \\right) \\\\\n",
    "&= \\gamma_0 - \\gamma_{|t - s|} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  For the random walk, assuming without loss of generality $t \\leq s$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\left(\\sum_{j=t+1}^s e_j \\right)^2 \\right] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\sum_{j=t+1}^s e_j^2 + \\sum_i \\sum_{j, j \\neq i} e_i e_j\\right] \\\\\n",
    "&= \\frac{1}{2} \\sum_{j=t+1}^s \\text{E} [e_j^2] + \\sum_i \\sum_{j, j \\neq i} \\text{E}[e_i] \\text{E}[e_j] \\\\\n",
    "&= \\frac{1}{2} (s - t) \\sigma_e^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while, for $t < s$, we can do $\\Gamma_{t, s} = \\Gamma_{s, t} = \\frac{1}{2} (t - s) \\sigma_e^2$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\Gamma_{s, t} = \\frac{1}{2} |s - t| \\sigma_e^2 $$\n",
    "\n",
    "and so a random walk is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.27**.  For a fixed, positive integer $r$ and constant $\\phi$, consider the time series defined by $Y_t = e_t + \\phi e_{t - 1} + \\phi^2 e_{t - 2} + \\cdots + \\phi^r e_{t - r}$.\n",
    "\n",
    "**(a)** Show that this process is stationary for any value of $\\phi$.\n",
    "\n",
    "**(b)** Find the autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] \n",
    "= \\text{E}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right] \n",
    "= \\sum_{j=0}^r \\phi^j \\text{E}[e_{t-j}] \n",
    "= \\sum_{j=0}^r \\phi^j \\cdot 0\n",
    "= 0\n",
    "$$\n",
    "\n",
    "which is constant (and does not depend on $t$).\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] \n",
    "= \\text{Var}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right]\n",
    "= \\sum_{j=0}^r \\phi^{2j} \\text{Var}[e_{t - j}] \n",
    "= \\sigma_e^2 \\sum_{j=0}^r \\phi^{2j}\n",
    "$$\n",
    "\n",
    "where this is a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\begin{cases}\n",
    "(r + 1) \\sigma_e^2 &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\sigma_e^2 \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "Finally, the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=0}^r \\phi^i e_{t - i}, \\sum_{j=0}^r \\phi^j e_{t - k - j} \\right] \\\\\n",
    "&= \\sum_{i=0}^r \\sum_{j=0}^r \\phi^{i + j}\\text{Cov}[e_{t - i}, e_{t - k - j} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The only non-zero terms in the sum occur when the indexes of both variables are the same, which is to say, $i = k + j$, with $i, j \\in [0, r]$.  Assuming $k \\leq r$, we can then write the somewhat simpler formula,\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - k}] = \\sigma_e^2 \\sum_{i=0}^r \\phi^{2i - k} = \\frac{\\sigma_e^2}{\\phi^k} \\sum_{i = 0}^r \\phi^{2i}\n",
    "$$\n",
    "\n",
    "which is again a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "(r + 1) \\frac{\\sigma_e^2}{\\phi^k} &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\frac{\\sigma_e^2}{\\phi^k} \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "As this is not a function of time either, the process $\\{ Y_t \\}$ is stationary for any value of $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As we already computed the autocovariance, the autocorrelation is relatively straightforward,\n",
    "\n",
    "$$ \\rho_k = \\gamma_k / \\gamma_0 $$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "\\phi^{-k} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.28 (Random cosine wave extended)**.  Suppose that\n",
    "\n",
    "$$ Y_t = R \\cos (2 \\pi (f t + \\Phi)) \\quad \\text{for } t = 0, \\pm 1, \\pm 2, \\dots $$\n",
    "\n",
    "where $0 < f < \\frac{1}{2}$ is a fixed frequency and $R$ and $\\Phi$ are uncorrelated random variables and with $\\Phi$ uniformly distributed on the interval (0, 1).\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$ for all $t$.\n",
    "\n",
    "**(b)** Show that the process is stationary with $\\gamma_k = \\frac{1}{2} \\text{E}[R^2] \\cos (2 \\pi f k)$.\n",
    "\n",
    "Hint: Use the calculations leading up to Equation (2.3.4), on page 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
