{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.21**.  Consider the model $Y_t = e_{t - 1} - e_{t - 2} + 0.5 e_{t - 3}$.\n",
    "\n",
    "**(a)**  Find the autocovariance function for this process.\n",
    "\n",
    "**(b)**  Show that this is a certain ARMA($p$, $q$) process in disguise.  That is, identify values for $p$ and $q$ and for the $\\theta$'s and $\\phi$'s such that the ARMA($p$, $q$) process has the same statistical properties as $\\{Y_t\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[e_{t - 1} - e_{t - 2} + 0.5 e_{t - 3}, e_{t-1-k} - e_{t-2-k} + 0.5 e_{t-3-k}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "2.25 \\sigma_e^2 &\\text{for } k = 0\\\\\n",
    "1.5 \\sigma_e^2 &\\text{for } k = 1 \\\\\n",
    "0.5 \\sigma_e^2 &\\text{for } k = 2 \\\\\n",
    "0 &\\text{for } k > 2\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  This is a MA(2) process; just define $W_t = Y_{t - 1}$ and note that $W_t = e_t - e_{t-1} + 0.5e_{t-2}$, where $\\theta_1 = -1, \\theta_2 = 0.5$.  Therefore, this is an ARMA($p$, $q$) process with $p = 0$, $q = 2$, and the provided values of $\\theta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.22**  Show that the statement \"The roots of $1 - \\phi_1 x - \\phi_2 x^2 - \\cdots - \\phi_p x^p = 0$ are greater than 1 in absolute value\" is equivalent to the statement \"The roots of $x^p - \\phi_1 x^{p-1} - \\phi_2 x^{p-2} - \\cdots - \\phi_p = 0$ are less than 1 in absolute value$.  (Hint: if $G$ is a root of one equation, is $1/G$ a root of the other?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  The result is almost immediate from the hint.  If $G$ is a root of $1 - \\phi_1 x - \\phi_2 x^2 - \\cdots - \\phi_p x^p = 0$,\n",
    "\n",
    "$$ 1 - \\phi_1 G - \\phi_2 G^2 - \\cdots - \\phi_p G^p = 0 $$\n",
    "\n",
    "so, dividing by $G^p$ (as we know $G \\neq 0$ since 0 is not a root),\n",
    "\n",
    "$$ G^{-p} - \\phi_1 G^{-p+1} - \\phi_2 G^{-p+2} - \\cdots - \\phi_p = 0 $$\n",
    "\n",
    "and so $G^{-1}$ is a root of $x^p - \\phi_1 x^{p-1} - \\phi_2 x^{p-2} - \\cdots - \\phi_p = 0$.  Therefore, if the absolute value of $G$ is greater than 1, then the absolute value of $G^{-1}$ is less than 1, and the first statement applies to all roots of the first equation if and only if the second statement applies to all roots of the second equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.23**.  Suppose that $\\{Y_t\\}$ is an AR(1) process with $\\rho_1 = \\phi$.  Define the sequence $\\{ b_t \\}$ as $b_t = Y_t - \\phi Y_{t+1}$.\n",
    "\n",
    "**(a)** Show that $\\text{Cov}[b_t, b_{t - k}] = 0$ for all $t$ and $k$.\n",
    "\n",
    "**(b)** Show that $\\text{Cov}[b_t, Y_{t + k}] = 0 $ for all $t$ and $k > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[b_t, b_{t - k}] &= \\text{Cov}[Y_t - \\phi Y_{t+1}, Y_{t-k} - \\phi Y_{t-k+1}] \\\\\n",
    "&= \\text{Cov}[Y_t, Y_{t-k}] - \\phi \\text{Cov}[Y_{t+1}, Y_{t-k}] - \\phi \\text{Cov}[Y_t, Y_{t-k+1}] + \\phi^2 \\text{Cov}[Y_{t+1}, Y_{t-k+1}] \\\\\n",
    "&= \\gamma_k - \\phi \\gamma_{k+1} - \\phi \\gamma_{k - 1} + \\phi^2 \\gamma_k \\\\\n",
    "&= \\gamma_0 (\\phi^k - \\phi \\phi^{k+1} - \\phi \\phi^{k-1} + \\phi^2 \\phi^k) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[b_t, Y_{t + k}] &= \\text{Cov}[Y_t - \\phi Y_{t+1}, Y_{t+k}] \\\\\n",
    "&= \\text{Cov}[Y_t, Y_{t+k}] - \\phi \\text{Cov}[Y_{t+1}, Y_{t+k}] \\\\\n",
    "&= \\gamma_k - \\phi \\gamma_{k-1} \\\\\n",
    "&= \\gamma_0 (\\phi^k - \\phi \\phi^{k-1}) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.24**.  Let $\\{e_t\\}$ be a zero-mean, unit-variance white noise process.  Consider a process that begins at time $t = 0$ and is defined recursively as follows.  Let $Y_0 = c_1 e_0$ and $Y_1 = c_2 Y_0 + e_1$.  Then let $Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t$ for $t > 1$ as in an AR(2) process.\n",
    "\n",
    "**(a)** Show that the process mean is zero.\n",
    "\n",
    "**(b)** For particular values of $\\phi_1$ and $\\phi_2$ within the stationary region for an AR(2) model, show how to choose $c_1$ and $c_2$ so that both $\\text{Var}[Y_0] = \\text{Var}[Y_1]$ and the lag 1 autocorrelation between $Y_1$ and $Y_0$ match that of a stationary AR(2) process with parameters $\\phi_1$ and $\\phi_2$.\n",
    "\n",
    "**(c)** Once the process $\\{Y_t\\}$ is generated, show how to transform it to a new process that has any desired mean and variance.  (This exercise suggests a convenient method for simulating AR(2) processes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Let's show the process has mean zero by induction.  \n",
    "\n",
    "Base cases: $Y_0$ and $Y_1$:\n",
    "\n",
    "- $ \\text{E}[Y_0] = \\text{E}[c_1 e_0] = c_1 \\text{E}[e_0] = 0$\n",
    "- $ \\text{E}[Y_1] = \\text{E}[c_2 Y_0 + e_1] = c_2 \\text{E}[Y_0] + \\text{E}[e_1] = c_2 \\cdot 0 + 0 = 0$\n",
    "\n",
    "Induction step: assuming $\\text{E}[Y_{t-2}] = \\text{E}[Y_{t-1}] = 0$,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t] = \\phi_1 \\text{E}[Y_{t-1}] + \\phi_2 \\text{E}[Y_{t-2}] + \\text{E}[e_t] = \\phi_1 \\cdot 0 + \\phi_2 \\cdot 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  We have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "&\\text{Var}[Y_0] = \\text{Var}[Y_1] \\\\\n",
    "\\Longleftrightarrow\\;& c_1^2 \\sigma_e^2 = c_2^2 c_1^2 \\sigma_e^2 + \\text{Var}[e_1] \\\\\n",
    "\\Longleftrightarrow\\;& c_1^2 (1 - c_2^2) = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\text{Cov}[Y_1, Y_0] = \\text{Cov}[c_2 Y_0 + e_1, Y_0] = c_2 \\text{Var}[Y_0] $$\n",
    "\n",
    "so $\\gamma_1 = c_2 \\gamma_0$, or $\\rho_1 = c_2$.  For an AR(2) process, $\\rho_1 = \\phi_1 / (1 - \\phi_2)$, so we can pick\n",
    "\n",
    "$$ c_2 = \\frac{\\phi_1}{1 - \\phi_2}\n",
    "\\quad \\text{and} \\quad\n",
    "c_1 = \\frac{1}{\\sqrt{1 - c_2^2}}$$\n",
    "\n",
    "to satisfy both conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The process can be transforming by shifting it by scaling it by the desired variance and then shifting it with the desired mean;\n",
    "\n",
    "$$ W_t = \\frac{Y_t \\sigma}{c_1} + \\mu $$\n",
    "\n",
    "has mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$ \\text{E}[W_t] = \\frac{\\sigma}{c_1}\\text{E}[Y_t] + \\mu = \\mu\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Var}[W_t] = \\frac{\\sigma^2}{c^2} \\text{Var}[Y_t] = \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.25**.  Consider an \"AR(1)\" process satisfying $Y_t = \\phi Y_{t-1} + e_t$ where $\\phi$ can be **any** number and $\\{e_t\\}$ is a white noise process such that $e_t$ is independent of the past $\\{Y_{t-1}, Y_{t-2}, \\dots\\}$.  Let $Y_0$ be a random variable with mean $\\mu_0$ and variance $\\sigma_0^2$.\n",
    "\n",
    "**(a)**  Show that for $t > 0$ we can write\n",
    "\n",
    "$$ Y_t = e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\phi^3 e_{t-3} + \\cdots + \\phi^{t-1} e_1 + \\phi^t Y_0 $$\n",
    "\n",
    "**(b)**  Show that for $t > 0$ we have $\\text{E}[Y_t] = \\phi^t \\mu_0$.\n",
    "\n",
    "**(c)**  Show that for $t > 0$\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\begin{cases}\n",
    "\\frac{1 - \\phi^{2t}}{1 - \\phi^2} \\sigma_e^2 + \\phi^{2t} \\sigma_0^2 &\\text{for } \\phi \\neq 1 \\\\\n",
    "t \\sigma_e^2 + \\sigma_0^2 &\\text{for } \\phi = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "**(d)**  Suppose now that $\\mu_0 = 0$.  Argue that, if $\\{Y_t\\}$ is stationary, we must have $\\phi \\neq 1$.\n",
    "\n",
    "**(e)**  Continuing to suppose that $\\mu_0 = 0$, show that, if $\\{ Y_t \\}$ is stationary, then $\\text{Var}[Y_t] = \\sigma_e^2 / (1 - \\phi^2)$ and so we must have $|\\phi| < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Both sides of the equation are the same for $t = 0$.  By induction, assuming the result holds for $t$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "Y_{t+1} = \\phi Y_t + e_{t+1} &= e_{t+1} + \\phi \\left( e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\cdots + \\phi^{t-1} e_1 + e^t Y_0 \\right) \\\\\n",
    "&= e_{t+1} + \\phi e_t + \\phi^2 e_{t-1} + \\phi^3 e_{t-2} + \\cdots + \\phi^t e_1 + \\phi^{t+1} Y_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the result holds for $t + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Using the result from (a),\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y_t] &= \\text{E}\\left[ \\sum_{k=0}^{t-1} \\phi^k e_{t - k} + \\phi^t Y_0 \\right] \\\\\n",
    "&= \\sum_{k=0}^{t-1} \\phi^k \\text{E}[e_{t-k}] + \\phi^t \\text{E}[Y_0] \\\\\n",
    "&= \\sum_{k=0}^{t-1} \\phi^k \\cdot 0 + \\phi^t \\mu_0 \\\\\n",
    "&= \\phi^t \\mu_0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Using the result from (a),\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Var}[Y_t] &= \\text{Var}\\left[ \\sum_{k=0}^{t-1} \\phi^k e_{t - k} + \\phi^t Y_0 \\right] \\\\\n",
    "&= \\sum_{k=0}^{t-1} \\phi^{2k} \\text{Var}[e_{t-k}] + \\phi^{2t} \\text{Var}[Y_0] \\\\\n",
    "&= \\sigma_e^2 \\left(\\sum_{k=0}^{t-1} \\phi^{2k} \\right)  + \\phi^t \\sigma_0^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the result follows, as either the sum of the finite geometric series (for $\\phi \\neq 1$) or the sum of $t$ equal terms (for $\\phi = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  The argument is the same as in Exercise 4.15:\n",
    "\n",
    "If $\\{Y_t\\}$ is stationary, then $Y_t = \\phi Y_{t-1} + e_t$ implies, taking the variance on both sides, that $\\gamma_0 = \\phi^2 \\gamma_0 + \\sigma_e^2$.  This is a contradiction if $\\phi^2 = 1$, as that would imply $\\sigma_e^2 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Similar to (d), assuming that $\\{Y_t\\}$ is stationary, we have $Y_t = \\phi Y_{t-1} + e_t$ implies $\\gamma_0 = \\phi^2 \\gamma_0 + \\sigma_e^2$, and so we must have $\\gamma_0 = \\sigma_e^2 / (1 - \\phi^2)$.  This implies $|\\phi| < 1$, otherwise this would mean that the variance is negative, which is a contradiction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
