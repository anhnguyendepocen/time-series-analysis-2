{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamental Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**. Suppose $\\text{E}[X] = 2$, $\\text{Var}[X] = 9$, $\\text{E}[Y] = 0$, $\\text{Var}[Y] = 4$, and $\\text{Corr}(X, Y) = 0.25$.  Find:\n",
    "\n",
    "**(a)** $\\text{Var}[X + Y]$\n",
    "\n",
    "**(b)** $\\text{Cov}[X, X + Y]$\n",
    "\n",
    "**(c)** $\\text{Corr}[X + Y, X - Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Note that $\\text{Cov}[X, Y] = \\text{Corr}[X, Y] \\sqrt{\\text{Var}[X] \\text{Var}[Y]} = 0.25 \\cdot \\sqrt{9 \\cdot 4} = 1.5$.  Then,\n",
    "\n",
    "$$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Cov}[X, X + Y] = \\text{Cov}[X, X] + \\text{Cov}[X, Y] = \\text{Var}[X] + \\text{Cov}[X, Y] = 9 + 1.5 = 10.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "Computing the covariance:\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 9 - 4 = 5 $$\n",
    "\n",
    "Computing the variance of $X + Y$:\n",
    "\n",
    "$$ \\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5 $$\n",
    "\n",
    "Computing the variance of $X - Y$:\n",
    "\n",
    "$$ \\text{Var}[X - Y] = \\text{Var}[X] + \\text{Var}[Y] - \\text{Cov}[X, Y] = 9 + 4 - 1.5 = 11.5 $$\n",
    "\n",
    "So,\n",
    "\n",
    "$$ \\text{Corr}[X + Y, X - Y] = \\frac{\\text{Cov}[X + Y, X - Y]}{\\sqrt{\\text{Var}[X + Y] \\text{Var}[X - Y]}} = \\frac{5}{\\sqrt{14.5 \\cdot 11.5}} \\approx 0.3872 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**.  If $X$ and $Y$ are dependent but $\\text{Var}[X] = \\text{Var}[Y]$, find $\\text{Cov}[X + Y, X - Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**.  Let $X$ have a distribution with mean $\\mu$ and variance $\\sigma^2$, and let $Y_t = X$ for all $t$.\n",
    "\n",
    "**(a)** Show that $\\{ Y_t \\}$ is strictly and weakly stationary.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Sketch a \"typical\" time plot of $\\{ Y_t \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By definition, the distribution of $Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}$ is $n$ identical copies of a sample drawn from $X$, which is the same as the distribution of $Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}$ for any $k$, therefore $\\{Y_t\\}$ is strictly stationary.\n",
    "\n",
    "The mean function is $\\mu$ for all $t$, so it is constant over time, and the autocovariance function is $\\gamma_{t, t - k} = \\text{Cov}[Y_t, Y_{t - k}] = \\text{Var}[X] = \\sigma^2$ is also constant for all time $t$ and lag $k$, and so $\\{Y_t\\}$ is weakly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a), the covariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\text{Cov}[Y_t, Y_s] = \\text{Var}[X] = \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  A \"typical\" time plot of $\\{ Y_t \\}$ is a constant series, with all values equal to some value drawn from the distribution of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: latex2exp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "require(ggplot2)\n",
    "require(latex2exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaAAAAHgCAMAAABkaTqFAAAAYFBMVEUAAAAAAP8zMzNNTU1o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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 240,
       "width": 720
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=4)\n",
    "\n",
    "X = rep.int(3, 10)\n",
    "nn = seq(1, 10)\n",
    "\n",
    "ggplot() + \n",
    "  geom_line(aes(x=nn, y=X), color='blue') +\n",
    "  geom_point(aes(x=nn, y=X), color='blue') +\n",
    "  xlab(TeX('t')) + ylab(TeX('Y_t')) +\n",
    "  theme_bw() + theme(text = element_text(size=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4**.  Let $\\{ e_t \\}$ be a zero mean white noise process.  Suppose that the observed process is $Y_t = e_t + \\theta e_{t - 1}$, where $\\theta$ is either 3 or 1/3.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $\\{ Y_t \\}$ both when $\\theta = 3$ and when $\\theta = 1/3$.\n",
    "\n",
    "**(b)** You should have discovered that the time series is stationary regardless of the value of $\\theta$ and that the autocorrelation functions are the same for $\\theta = 3$ and $\\theta = 1/3$.  For simplicity, suppose that the process mean is known to be zero and the variance of $Y_t$ is known to be 1.  You observe the series $\\{Y_t\\}$ for $t = 1, 2, \\dots, n$ and suppose that you can produce good estimates of the autocorrelations $\\rho_k$.  Do you think that you could determine which value of $\\theta$ is correct (3 or 1/3) based on the estimate of $\\rho_k$?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The variance is:\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t + \\theta e_{t - 1}] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t - 1}] = (1 + \\theta^2) \\sigma_e^2 $$\n",
    "\n",
    "Also\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - 1} + \\theta e_{t - 2}] = \\theta \\text{Var}[e_{t-1}] = \\theta \\sigma_2^2\n",
    "$$\n",
    "\n",
    "and for element pairs with lag $k > 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - k} + \\theta e_{t - k - 1}] = 0 $$\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\begin{cases}\n",
    "(1 + \\theta^2) \\sigma_e^2 &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta \\sigma_e^2         &\\text{for } | t - s | = 1 \\\\\n",
    "0                         &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1                            &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta / (1 + \\theta^2)      &\\text{for } | t - s | = 1 \\\\\n",
    "0                            &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that $\\theta / (1 + \\theta^2) = 0.3$ for both $\\theta = 3$ and $\\theta = 1/3$ (this likely being the point of the exercise), so in either scenario the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1        &\\text{for } | t - s | = 0 \\\\\n",
    "0.3      &\\text{for } | t - s | = 1 \\\\\n",
    "0        &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a) and stated on the exercise, the series will have the same autocorrelation whether $\\theta = 3$ or $\\theta = 1/3$, so estimates of this property cannot be used to distinguish between the two candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5**.  Suppose $Y_t = 5 + 2t + X_t$, where $\\{ X_t \\}$ is zero-mean stationary series with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{ Y_t \\}$ stationary?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "\n",
    "$$ \\mu_t = \\text{E}[Y_t] = \\text{E}[5 + 2t + X_t] = 5 + 2t + \\text{E}[X_t] = 5 + 2t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The covariance for terms with lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[5 + 2t + X_t, 5 + 2(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is the same as the autocovariance function for $\\{X_t\\}$, $\\gamma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The series $\\{Y_t\\}$ is not stationary because the mean is not constant over time -- there is a time drift term, $2t$, in the mean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.6**.  Let $\\{ X_t \\}$ be a stationary time series, and let \n",
    "\n",
    "$$ Y_t = \\begin{cases}\n",
    "X_t &\\text{for } t \\text{ odd} \\\\\n",
    "X_t + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "**(a)**  Show that $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$ for all lags $k$.\n",
    "\n",
    "**(b)**  Is $\\{ Y_t \\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For every possible parity combination of $t$ and $k$ we have:\n",
    "\n",
    "- Even $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Even $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "\n",
    "Since $\\{X_t\\}$ is stationary, $\\text{Cov}[X_t, X_{t - k}]$ is free of $t$, and so $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No, as its mean function is not constant over time.  Given that $\\{ X_t \\}$ is stationary, it has a constant mean $\\overline{X}$, and we have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\begin{cases}\n",
    "\\overline{X} &\\text{for } t \\text{ odd} \\\\\n",
    "\\overline{X} + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "is not constant over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.7**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)**  Show that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary by finding the mean and the autocovariance function for $\\{ W_t \\}$.\n",
    "\n",
    "**(b)**  Show that $U_t = \\nabla^2 Y_t = \\nabla[Y_t - Y_{t - 1}] = Y_t - 2 Y_{t - 1} + Y_{t - 2}$ is stationary.  (You need not find the mean and autocovariance function for $\\{U_t\\}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Since $\\{Y_t\\}$ has a constant mean $\\overline{Y}$ over time, the mean of $W_t$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \\text{Var}[W_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\gamma_1 $$\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t,  Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "therefore the autocovariance function for $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since the autocovariance function is free of $k$ and the mean is constant over time, then $\\{W_t\\} is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the result of (a) to the series $\\{ W_t \\}$ rather than $ \\{ Y_t \\}$, we get that the series $\\{U_t\\} = \\{ \\nabla W_t \\}$ must also be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.8**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.  Show that for any positive integer $n$ and any constants $c_1, c_2, \\dots c_n$ the process $\\{ W_t \\}$ defined by $W_t = c_1 Y_t + c_2 Y_{t - 1} + \\cdots + c_n Y_{t - n + 1}$ is stationary.  (Note that Exercise 2.7 is a special case of this result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  If the mean of $\\{Y_t\\}$ is $\\overline{Y}$, then the mean of $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1} \\right] = \\sum_{i=1}^n c_i \\text{E}[Y_{t - i + 1}] = \\overline{Y} \\sum_{i=1}^n c_i $$\n",
    "\n",
    "which is constant over time.\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1}, \\sum_{j=1}^n c_i Y_{t - j + 1 - k}\\right] \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and since the difference in the indices of $\\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]$ does not depend on $t$, no matter which index is largest, then the overall expression is always a linear combination of the autocovariance function $\\gamma_m$ for $0 \\leq m \\leq n + k - 1$,\n",
    "\n",
    "$$ \\text{Cov}[W_t, W_{t - k}] = \\sum_{m=0}^{n + k - 1} d_{m, k} \\gamma_m $$\n",
    "\n",
    "for some constants $d_{m, k}$ that do not depend on $t$.  Therefore, $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.9**.  Suppose $Y_t = \\beta_0 + \\beta_1 t + X_t$, where $\\{X_t\\}$ is a zero-mean stationary series with autocovariance function $\\gamma_k$ and $\\beta_0$ and $\\beta_1$ are constants.\n",
    "\n",
    "**(a)**  Show that $\\{ Y_t \\}$ is not stationary but that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary.\n",
    "\n",
    "**(b)**  In general, show that if $Y_t = \\mu_t + X_t$, where $\\{ X_t \\}$ is a zero-mean stationary series and $\\mu_t$ is a polynomial in $t$ of degree $d$, then $\\nabla^m Y_t = \\nabla (\\nabla^{m - 1} Y_t)$ is stationary for $m \\geq d$ and nonstationary for $0 \\leq m \\leq d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\beta_0 + \\beta_1 t + X_t] = \\beta_0 + \\beta_1 t + \\overline{X} $$\n",
    "\n",
    "so the mean of $Y_t$ is not constant in time, and so $\\{Y_t\\}$ is not stationary.\n",
    "\n",
    "On the other hand, the mean of $\\{W_t\\}$ is constant in time,\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\beta_1 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Var}[W_t] &= \\text{Var}[Y_t - Y_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_1 + X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}{X_t} + \\text{Var}[X_{t - 1}] - 2 \\text{Cov}[X_t, X_{t - 1}] \\\\\n",
    "&= 2\\gamma_0 - 2\\gamma_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while the covariance of $W_t$ for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}, \n",
    "\\beta_0 + \\beta_1 (t - k) + X_{t - k} - \\beta_0 - \\beta_1 (t - k - 1) - X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_1 + X_t - X_{t - 1}, \\beta_1 + X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t - X_{t - 1}, X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t, X_{t - k}] - \\text{Cov}[X_t, X_{t - k - 1}] - \\text{Cov}[X_{t - 1}, X_{t - k - 1}] + \\text{Cov}[X_{t - 1}, X_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the autocovariance function for $\\{W_t\\}$ does not depend on $t$,\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since $\\{W_t\\}$ has a constant mean in time and an autocovariance function that does not depend on time, it is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  \n",
    "\n",
    "**Lemma 2.9.1**: if $\\{Y_t\\}$ is stationary, then $\\{ \\nabla Y_t \\}$ must also be stationary.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "- The mean of $\\{ \\nabla Y_t \\}$ constant (and zero):\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "- Assuming $\\{Y_t\\}$ has autocovariance function $\\gamma_k$, the variance of $\\{ \\nabla Y_t \\}$ is:\n",
    "\n",
    "  $$ \\text{Var}[ \\nabla Y_t ] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}{Y_t} + \\text{Var}[Y_{t - 1}] - 2 \\text{Cov}[Y_t, Y_{t - 1}] = 2\\gamma_0 - 2 \\gamma_1 $$\n",
    "\n",
    "  and the autocovariance for lag $k > 0$ is\n",
    "  \n",
    "  $$ \n",
    "  \\begin{align}\n",
    "  \\text{Cov}[\\nabla Y_t, \\nabla Y_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "  &= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t, Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "Therefore $\\{\\nabla Y_t\\}$ has a constant mean and an autocovariance function that does not depend on time, and so it is stationary.\n",
    "\n",
    "**Lemma 2.9.2**: If $\\mu_t$ is a polynomial of degree $d$, then $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1})$ is a polynomial of degree $d - 1$, for $1 \\leq k \\leq d$.\n",
    "\n",
    "**Proof**: Let $\\mu_t = \\sum_{j=0}^d c_j t^j$, for constants $c_j$.  \n",
    "\n",
    "- For $k = 1$, $\\nabla \\mu_t = \\mu_t - \\mu_{t - 1} = \\sum_{j=1}^d c_j (t^j - (t - 1)^j) $.  In each term, the coefficients of $t^j$ in $t^j$ and $(t - 1)^j$ cancel out, so $t^j - (t - 1)^j$ is a polynomial of degree $j - 1$, and the overall expression is a polynomial of degree $d - 1$.\n",
    "- For $k > 1$, $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1}) = \\nabla (a_t - a_{t - 1})$, where $a_t$ is a polynomial of degree $d - k + 1$ by induction, therefore $\\nabla^k \\mu_t$ is a polynomial of degree $d - k$.\n",
    "\n",
    "Now, we can prove the result from (b) by induction.  \n",
    "\n",
    "- Base case $d = 0$:  This is equivalent to Lemma 2.9.1.\n",
    "\n",
    "- Base case $d = 1$:  This is equivalent to the result in (a).\n",
    "\n",
    "- Induction step:\n",
    "  - If $m < d$, then $\\nabla_m Y_t$ is a polynomial $A$ in $t$ with degree $d - m$, plus a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$.  Therefore the expected value of $\\nabla_m Y_t$ is  $\\text{E}[A(t) - A(t - 1) + \\sum c_j X_j] = A(t) - A(t - 1) + \\overline{X} \\sum c_j$, which is a non-constant function of $t$.  Therefore the mean is not constant in time, and so $\\{ \\nabla^m Y_t \\}$ is not stationary.\n",
    "  - If $m > d$, then $\\nabla_m Y_t$ is a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times zeroes out the polynomial component).  Therefore, from Lemma 2.9.1, $\\{ \\nabla^m Y_t \\}$ is stationary.\n",
    "  - If $m = d$, then $\\nabla_m Y_t$ is a zero degree polynomial (a constant) plus a linear combination of of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times to bring the degree down to exactly 0).  Therefore, the expectation of $\\nabla^m Y_t$ is that constant plus $\\overline{X} \\sum c_j$, and so it does not change with time.  The zero-polynomial constant only scales up the covariance terms by a constant factor, and so the autocovariance also does not depend on the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.10**.  Let $\\{X_t\\}$ be a zero-mean, unit-variance stationary process with autocorrelation function $\\rho_k$.  Suppose that $\\mu_t$ is a nonconstant function and that $\\sigma_t$ is a positive-valued nonconstant function.  The observed series is formed as $Y_t = \\mu_t + \\sigma_t X_t$.\n",
    "\n",
    "**(a)**  Find the mean and the covariance function for the $\\{ Y_t \\}$ process.\n",
    "\n",
    "**(b)**  Show that the autocorrelation function for the $\\{ Y_t \\}$ process depends only on the time lag.  Is the $\\{Y_t\\}$ process stationary?\n",
    "\n",
    "**(c)**  Is it possible to have a time series with a constant mean and with $\\text{Corr}[Y_t, Y_{t - k}]$ free of $t$ but with $\\{Y_t\\}$ not stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\mu_t + \\sigma_t X_t] = \\mu_t + \\sigma_t \\text{E}[X_t] = \\mu_t $$\n",
    "\n",
    "and the covariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}[\\mu_t + \\sigma_t X_t, \\mu_s + \\sigma_s X_s] = \\sigma_t \\sigma_s \\text{Cov}[X_t, X_s] = \\sigma_t \\sigma_s \\rho_{t - s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The autocorrelation for the $\\{ Y_t \\}$ process is\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_t \\sigma_{t - k} \\rho_k}{\\sqrt{\\sigma_t^2 \\sigma_{t - k}^2}} = \\rho_k $$\n",
    "\n",
    "that is, it is the same as the autocorrelation for the $\\{X_t\\}$ process, and it depends only on the time lag.\n",
    "\n",
    "Note that this does not make the process stationary, both because it has nonconstant mean $\\mu_t$ and because stationarity requires time-independent autocovariance, not time-independent autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Yes -- for example, take $\\{X_t\\}$ as some series with more than two distinct non-zero autocorrelation values, make $\\mu_t = 0$ and $\\sigma_t = t$.  Then, even though $\\{Y_t\\}$ has constant mean 0, the autocovariance depends on the time,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\sigma_t \\sigma_{t - k} \\rho_k = t(t-k) \\rho_k$$\n",
    "\n",
    "despite that $\\text{Corr}[Y_t, Y_{t - k}] = \\rho_k$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.11**.  Suppose $\\text{Cov}[X_t, X_{t - k}] = \\gamma_k$ is free of $t$ but that $\\text{E}[X_t] = 3t$.\n",
    "\n",
    "**(a)** Is $\\{ X_t \\}$ stationary?\n",
    "\n",
    "**(b)** Let $Y_t = 7 - 3t + X_t$.  Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  No.  By definition, a series with a time-dependant mean is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  We have that $\\{ Y_t \\}$ has constant mean,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[7 - 3t + X_t] = 7 - 3t + \\text{E}[X_t] = 7 $$\n",
    "\n",
    "and its autocovariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[7 - 3t + X_t, 7 - 3(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "which is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.12**.  Suppose that $Y_t = e_t - e_{t - 12}$.  Show that $\\{Y_t\\}$ is stationary and that, for $k > 0$, its autocorrelation function is nonzero only for lag $k = 12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - e_{t - 12}] = \\text{E}[e_t] - \\text{E}[e_{t - 12}] = \\overline{e} - \\overline{e} = 0 $$\n",
    "\n",
    "so the series has constant mean.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - e_{t - 12}] = \\text{Var}[e_t] + \\text{Var}[e_{t - 12}] = 2 \\sigma_e^2 $$\n",
    "\n",
    "and for $k > 0$, its autocovariance function is \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - e_{t - 12}, e_{t - k} - e_{t - k - 12}] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\text{Cov}[e_{t - 12}, e_{t - k}] - \\text{Cov}[e_t, e{t - k - 12}] + \\text{Cov}[e_t, e_{t - k - 12}] \\\\\n",
    "&= -\\text{Cov}[e_{t - 12}, e_{t - k}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is zero if $k \\neq 12$ and $-\\sigma_e^2$ otherwise.\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "2 \\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "-\\sigma_e^2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "-1/2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the mean is constant and the autocovariance does not depend on the time, the series is stationary.  We have also shown that the only lag with a non-zero autocorrelation value is for $k = 12$, when the autocorrelation term is -1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.13**.  Let $Y_t = e_t - \\theta (e_{t - 1})^2$.  For this exercise, assume that the white noise series is normally distributed.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $Y_t$.\n",
    "\n",
    "**(b)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - \\theta (e_{t - 1})^2] = \\text{E}[e_t] - \\theta \\text{E}[e_{t-1}^2] = -\\theta \\sigma_e^2 $$\n",
    "\n",
    "The variance of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - \\theta (e_{t - 1})^2] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t-1}^2] = \\sigma_e^2 + \\theta^2 \\text{E}[e_{t-1}^4] = \\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 $$\n",
    "\n",
    "where we used the fact that the fourth moment of a normal distribution $N(0, \\sigma^2)$ is $\\text{E}[X^4] = 3 \\sigma^4$.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k > 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - k} - \\theta (e_{t - k - 1})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\theta \\text{Cov}[e_t, (e_{t - k - 1})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - k}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - k - 1})^2] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since all random variables within the covariances are independent.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k = 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - 1} - \\theta (e_{t - 2})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - 1}] - \\theta \\text{Cov}[e_t, (e_{t -2})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - 2})^2] \\\\\n",
    "&= - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] \\\\\n",
    "&= -\\theta \\left( \\text{E}[e_{t - 1}^3] - \\text{E}[e_{t - 1}^2] \\text{E}[e_{t - 1}]\\right) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the fact that the third moment of a normal distribution  $N(0, \\sigma^2)$ is $\\text{E}[X^3] = 0$ and the first moment of the distribution is its mean, which is also zero.\n",
    "\n",
    "Therefore, the autocovariance of $\\{Y_t\\}$ is\n",
    "\n",
    "$$\n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation is\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  The series has a constant mean $\\overline{Y} = \\theta \\sigma_e^2$, and an autocovariance which does not depend on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.14**.  Evaluate the mean and the covariance function for each of the following processes.  In each case, determine whether or not the process is stationary.\n",
    "\n",
    "**(a)**  $Y_t = \\theta_0 + t e_t$.\n",
    "\n",
    "**(b)**  $W_t = \\nabla Y_t$, where $Y_t$ is given in part (a).\n",
    "\n",
    "**(c)**  $Y_t = e_t e_{t - 1}$.  (You may assume that $\\{e_t\\}$ is normal white noise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\theta_0 + t e_t] = \\theta_0 + t \\text{E}[e_t] = \\theta_0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[\\theta_0 + t e_t] = t^2 \\text{Var}[e_t] = t^2 \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\theta_0 - \\theta_0 = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[\\nabla Y_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}[Y_t] + \\text{Var}[Y_{t - 1}] = t^2 \\sigma_e^2 + (t - 1)^2 \\sigma_e^2 = (2t^2 + 2t - 1) \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t e_{t-1}] = \\text{E}[e_t] \\text{E}[e_{t - 1}] = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t e_{t - 1}] = \\text{E}[e_t^2 e_{t - 1}^2] = \\text{E}[e_t^2] \\text{E}[ e_{t - 1}^2] = \\sigma_e^2 $$\n",
    "\n",
    "The autocorrelation of the process with lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{E}[Y_t Y_{t - k}] - \\text{E}[Y_t] \\text{E}[Y_{t - k}]  \\\\\n",
    "&= \\text{E}[e_t e_{t - 1} e_{t - k} e_{t - k - 1}] - 0 \\cdot 0 \\\\\n",
    "&= \\text{E}[e_t] \\text{E}[e_{t - 1} e_{t - k} e_{t - k - 1}] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance of the process is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the process has a constant mean and autocovariance that is free of time $t$, the process is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.15**.  Suppose that $X$ is a random variable with zero mean.  Define a time series by $Y_t = (-1)^t X$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the covariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[(-1)^t X] = (-1)^t \\text{E}[X] = (-1)^t \\cdot 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{E}[Y_t^2] - \\text{E}[Y_t]^2 = \\text{E}[(-1)^{2t}X^2]  - 0^2= \\text{E}[X^2] = \\text{Var}[X] = \\sigma_X^2 $$\n",
    "\n",
    "The covariance for even lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = \\sigma_X^2 $$\n",
    "\n",
    "and the covariance for odd lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, -Y_t] = -\\text{Var}[Y_t] = -\\sigma_X^2 $$\n",
    "\n",
    "Therefore the covariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_{t, s} = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } |t - s| \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } |t - s| \\text{ odd}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Yes.  The mean is constant, and the autocovariance function is $t$ free, depending only on the lag:\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } k \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } k \\text{ odd} \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.16**.  Suppose $Y_t = A + X_t$, where $\\{X_t\\}$ is stationary and $A$ is random but independent of $\\{ X_t \\}$.  Find the mean and covariance function for $\\{Y_t\\}$ in terms of the mean and autocovariance function for $\\{ X_t \\}$ and the mean and variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The mean is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[A + X_t] = \\text{E}[A] + \\text{E}[X_t] = \\mu_A + \\mu_X $$\n",
    "\n",
    "where $\\mu_A, \\mu_X$ are the means of $A$ and $\\{X_t\\}$.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[A + X_t] = \\text{Var}[A] + \\text{Var}[X_t] = \\sigma_A^2 + \\sigma_X^2 $$\n",
    "\n",
    "The covariance is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_s] &= \\text{Cov}[A + X_t, A + X_s]\n",
    "&= \\text{Cov}[A, A] + \\text{Cov}[A, X_s] + \\text{Cov}[X_t, A] + \\text{Cov}[X_t, X_s] \\\\\n",
    "&= \\sigma_A^2 + \\gamma_{t, s}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\sigma_A^2 + \\gamma_k $$\n",
    "\n",
    "where $\\gamma_k$ is the autocovariance function for $\\{X_t\\}$ and $\\sigma_A^2$ is the variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.17**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Let $\\overline{Y} = \\frac{1}{n} \\sum_{t=1}^n Y_t$.  Show that\n",
    "\n",
    "$$ \\text{Var}[\\overline{Y}] = \\frac{\\gamma_0}{n} + \\frac{2}{n} \\sum_{k=1}^{n -1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k \n",
    "= \\frac{1}{n} \\sum_{k=-n+1}^{n - 1} \\left(1 - \\frac{|k|}{n} \\right) \\gamma_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have\n",
    "\n",
    "$$ \n",
    "\\text{Var}[\\overline{Y}] = \\text{Var}\\left[ \\frac{1}{n} \\sum_{t=1}^n Y_i \\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\text{Cov}[Y_i, Y_j] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|j - i|}\n",
    "$$\n",
    "\n",
    "The term with lag $k = |j - i|$ appears $2(n - k)$ times, as that is the number of ways of selecting $i, j$ in the summation intervals to produce that lag.  The equalities in the result follow as a consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.18**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Define the sample variance as $ S^2 = \\frac{1}{n - 1} \\sum_{t=1}^n (Y_t - \\overline{Y})^2$.\n",
    "\n",
    "**(a)** First show that $\\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t = 1}^n (Y_t - \\overline{Y})^2 + n(\\overline{Y} - \\mu)^2$.\n",
    "\n",
    "**(b)** Use part (a) to show that\n",
    "\n",
    "**(c)**  $$ \\text{E}[S^2] = \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}] = \\gamma_0 - \\frac{2}{n - 1} \\sum_{k=1}^{n - 1} \\left( 1 - \\frac{n}{k} \\right) \\gamma_k $$\n",
    "\n",
    "(Use the results of Exercise 2.17 for the last expression.)\n",
    "\n",
    "**(d)** It $\\{Y_t \\}$ is a white noise process with variance $\\gamma_0$, show that $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 =  \\sum_{t=1}^n \\left((Y_t - \\overline{Y}) + (\\overline{Y} - \\mu)\\right)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + \\sum_{t=1}^n (\\overline{Y} - \\mu)^2 + 2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) $$\n",
    "\n",
    "But the last term is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) &= 2 (\\overline{Y} - \\mu) \\sum_{t=1}^n (Y_t - \\overline{Y}) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) \\left( \\left( \\sum_{t=1}^n Y_t \\right) - n \\overline{Y} \\right) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) ( n \\overline{Y} - n \\overline{Y} ) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the result follows,\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + n (\\overline{Y} - \\mu)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b, c)** We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[S^2] &= \\frac{1}{n - 1} \\text{E} \\left[ \\sum_{t=1}^n (Y_t - \\overline{Y})^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 - n (\\overline{Y} - \\mu)^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 \\right] - n (\\overline{Y} - \\mu)^2 \\right) \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{Var}[Y_t] - n \\text{Var}[\\overline{Y}] \\right) \\\\\n",
    "&= \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the second equality follows directly by replacing $\\text{Var}[\\overline{Y}]$ with the expression in the result of Exercise 2.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  If $\\{Y_t\\}$ is a white noise process with variance $\\gamma_0$, the autocovariance function is 0 for any positive lags.  Replacing these in the second expression for $\\text{E}[S^2]$ in the previous item, we get $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.19**.  Let $Y_1 = \\theta_0 + e_1$, and then for $t > 1$ define $Y_t$ recursively by $Y_t = \\theta_0 + Y_{t - 1} + e_t$.  Here $\\theta_0$ is a constant.  The process $\\{Y_t\\}$ is called a **random walk with drift**.\n",
    "\n",
    "**(a)** Show that $Y_t$ may be rewritten as $Y_t = t\\theta_0 + e_t + e_{t - 1} + \\cdots + e_1 $.\n",
    "\n",
    "**(b)** Find the mean function for $Y_t$.\n",
    "\n",
    "**(c)** Find the autocovariance function for $Y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The result is true for $t = 1$.  Assuming by induction it is valid for $t - 1$,\n",
    "\n",
    "$$ Y_t = \\theta_0 + Y_{t - 1} + e_t = \\theta_0 + (t - 1)\\theta_0 + \\sum_{i=1}^{t - 1} e_i + e_t = t \\theta_0 + \\sum_{i=1}^t e_i $$\n",
    "\n",
    "and so the result holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ t \\theta_0 + \\sum_{i=1}^t e_i \\right] = t \\theta_0 + \\sum_{i=1}^t \\text{E}[e_i] = t \\theta_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[t \\theta_0 + \\sum_{i=1}^t e_i\\right] = \\sum_{i=1}^t \\text{Var}[e_i] = t \\sigma_e^2 $$\n",
    "\n",
    "and the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[t\\theta_0 + \\sum_{i=1}^t e_i, (t - k)\\theta_0 + \\sum_{j=1}^{t - k} e_j \\right]  \\\\\n",
    "&= \\sum_{i=1}^t \\sum_{j=1}^{t - k} \\text{Cov}[e_i, e_j]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The covariance terms above are non-zero only for $i = j$, and so\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = (t - k) \\sigma_e^2 $$\n",
    "\n",
    "Therefore the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = t \\sigma_e^2 \\quad \\text{for } 1 \\leq t \\leq s $$\n",
    "\n",
    "This is exactly the same as the autocovariance function for the random walk, but now the mean is not constant -- it has a drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.20**.  Consider the standard random walk model where $Y_t = Y_{t - 1} + e_t$ with $Y_1 = e_t$.\n",
    "\n",
    "**(a)** Use the representation of $Y_t$ above to show that $\\mu_t = \\mu_{t - 1}$ for $t > 1$ with initial condition $\\mu_1 = \\text{E}[e_1] = 0$.  Hence show that $\\mu_t = 0$ for all $t$.\n",
    "\n",
    "**(b)** Similarly, show that $\\text{Var}[Y_t] = \\text{Var}[Y_{t - 1}] + \\sigma_e^2$ for $t > 1$ with $\\text{Var}[Y_1] = \\sigma-e^2$ and hence $\\text{Var}[Y_t] = t \\sigma_e^2$.\n",
    "\n",
    "**(c)** For $0 \\leq t \\leq s$, use $Y_s = Y_t + e_{t + 1} + e_{t + 2} + \\cdots + e_s$ to show that $\\text{Cov}[Y_t, Y_s] = \\text{Var}[Y_t]$ and, hence, that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By induction,\n",
    "\n",
    "$$\\mu_t = \\text{E}[Y_t] = \\text{E}[Y_{t - 1} + e_t] = \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = \\mu_{t - 1} + 0 = \\mu_{t - 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[Y_{t - 1} + e_t] = \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = (t - 1)\\sigma_e^2 + \\sigma_e^2 = t \\sigma_ e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] + 0 = t \\sigma_e^2$$\n",
    "\n",
    "For the case of $t > s$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.21**.  For a random walk with random starting value, let $Y_t = Y_0 + e_t + e_{t - 1} + \\cdots + e_1$ for $t > 0$, where $Y_0$ has a distribution with mean $\\mu_0$ and variance $\\sigma_0^2$.  Suppose further that $Y_0, e_1, \\dots, e_t$ are independent.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = \\mu_0$ for all $t$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = t \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(c)** Show that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(d)** Show that \n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} \\quad \\text{for } 0 \\leq t \\leq s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{E}[Y_0] + \\sum_{i=1}^t \\text{E}[e_i] = \\mu_0 + 0 = \\mu_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{Var}[Y_0] + \\sum_{i=1}^t \\text{Var}[e_i] = \\sigma_0^2 + t\\sigma_e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] = t\\sigma_e^2 + \\sigma_0^2 $$\n",
    "\n",
    "For the case where $s > t$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  Assuming $0 \\leq t \\leq s$, we have\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\frac{\\text{Cov}[Y_t, Y_s]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_s]}} = \n",
    "\\frac{t \\sigma_e^2 + \\sigma_0^2}{\\sqrt{(t \\sigma_e^2 + \\sigma_0^2)(s \\sigma_e^2 + \\sigma_0^2)}}\n",
    "= \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.22**.  Let $\\{e_t\\}$ be a zero-mean white noise process, and let $c$ be a constant with $|c| < 1$.  Define $Y_t$ recursively by $Y_t = c Y_{t - 1} + e_t$ with $Y_1 = e_1$.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = \\sigma_e^2 (1 + c^2 + c^4 + \\cdots + c^{2t - 2})$.  Is $\\{Y_t\\}$ stationary?\n",
    "\n",
    "**(c)** Show that\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-1}] = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "and, in general,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-k}] = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "Hint:  Argue that $Y_{t - 1}$ is independent of $e_t$.  Then, use\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[cY_{t - 1} + e_t, Y_{t - 1}] $$\n",
    "\n",
    "**(d)**  For large $t$, argue that\n",
    "\n",
    "$$ \\text{Var}[Y_t] \\approx \\frac{\\sigma_e^2}{1 - c^2} \n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Corr}[Y_t, Y_{t - k}] \\approx c^k\n",
    "\\quad \\text{for } k > 0 $$\n",
    "\n",
    "so that $\\{ Y_t \\}$ could be called **asymptotically stationary**.\n",
    "\n",
    "**(e)** Suppose now that we alter the initial condition and put $Y_1 = \\frac{e_1}{\\sqrt{1 - c^2}}$.  Show that now $\\{Y_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have $\\text{E}[Y_1] = \\text{E}[e_1] = 0$.  By induction,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[cY_{t - 1} + e_t] = c \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = c \\cdot 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  We have $\\text{Var}[Y_1] = \\text{Var}[e_1] = \\sigma_e^2$.  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[c Y_{t - 1} + e_t] = c^2 \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = c^2 \\left( \\sigma_e^2 \\sum_{j = 0}^{t - 2} c^{2j} \\right) + \\sigma_e^2 = \\sigma_e^2 \\sum_{j=0}^{t - 1} c^{2j} $$\n",
    "\n",
    "Since this depends on the value of $t$, the autocovariance is dependent on $t$ for lag 0, and so $\\{Y_t\\}$ is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have:\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - 1}] = c \\text{Var}[Y_{t - 1}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - 1}] = \\frac{ c \\text{Var}[Y_{t - 1}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - 1}]} } = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "In general\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k} = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = \\cdots = c^k \\text{Cov}[Y_{t - k}, Y_{t - k}] = c^k \\text{Var}[Y_{t - k}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{ c^k \\text{Var}[Y_{t - k}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]} } = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  For large $t$ -- or more precisely, on the limit as $t \\rightarrow \\infty$, the following geometric sum is\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\sum_{j = 0}^{t - 2} c^{2j} = \\frac{1}{1 - c^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Var}[Y_t] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Corr}[Y_t, Y_{t - k}] = \\lim_{t \\rightarrow \\infty} c^k \\sqrt{\\frac{Y_{t - k}}{Y_t}} = c^k $$\n",
    "\n",
    "These values at the limit do not depend on $t$, which aligns with the definition of $\\{Y_t\\}$ being asymptotically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** The mean of the new initial value is still 0, so the previous results on the mean being 0 still hold.\n",
    "\n",
    "The variance now becomes\n",
    "\n",
    "$$ \\text{Var}[Y_1] = \\frac{1}{1 - c^2} \\text{Var}[e_1] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and the autocovariance becomes, by induction,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = c \\left( \\frac{c^{k-1}}{1 - c^2} \\right) = \\frac{c^k}{1 - c^2} $$\n",
    "\n",
    "The conditions for stationarity are now satisfied, since the mean is still constant and the autocorrelation function is free of $t$, instead depending on the lag,\n",
    "\n",
    "$$ \\gamma_k = \\frac{c^k}{1 - c^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.23**. Two processes $\\{Z_t\\}$ and $\\{Y_t\\}$ are said to be **independent** if for any time points $t_1, t_2, \\dots, t_m$ and $s_1, s_2, \\dots, s_n$ the random variables $\\{Z_{t_1}, Z_{t_2}, \\dots, Z_{t_m}\\}$ are independent of the random variables $\\{ S_{s_1}, S_{s_2}, \\dots, S_{s_n}\\}$.  Show that if $\\{Z_t\\}$ and $\\{Y_t\\}$ are independent stationary processes, then $W_t = Z_t + Y_t$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Z_t] + \\text{E}[Y_t] = \\mu_Z + \\mu_Y $$\n",
    "\n",
    "and so the mean of $\\{W_t\\}$ does not depend on time.\n",
    "\n",
    "Let $\\alpha_k$ be the autocorrelation function of the $\\{Z_t\\}$ and $\\beta_k$ be the autocorrelation function of the $\\{Y_t\\}$.  We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Z_t + Y_t, Z_{t - k} + Y_{t - k}] \\\\\n",
    "&= \\text{Cov}[Z_t, Z_{t - k}] + \\text{Cov}[Z_t, Y_{t - k}] + \\text{Cov}[Z_{t - k}, Y_t] + \\text{Cov}[Y_t. Y_{t - k}] \\\\\n",
    "&= \\alpha_k + \\beta_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $\\text{Cov}[Z_t, Y_{t - k}] = \\text{Cov}[Z_{t - k}, Y_t]$ from the independence assumption.\n",
    "\n",
    "Therefore, the autocorrelation function of $\\{W_t\\}$ is free of $t$, and since the mean of $\\{W_t\\}$ is constant $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.24**.  Let $\\{X_t\\}$ be a time series in which we are interested.  However, because the measurement process itself is not perfect, we actually observe $Y_t = X_t + e_t$.  We assume that $\\{X_t\\}$ and $\\{e_t\\}$ are independent processes.  We call $X_t$ the **signal** and $e_t$ the **measurement noise** or **error process**.\n",
    "\n",
    "If $\\{X_t\\}$ is stationary with autocorrelation function $\\rho_k$, show that $\\{Y_t\\}$ is also stationary with\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} \\quad \\text{for } k \\geq 1 $$\n",
    "\n",
    "We call $\\sigma_X^2 / \\sigma_e^2$ the **signal-to-noise ratio**, or SNR.  Note that the larger the SNR, the closer the autocorrelation function of the observed process $\\{Y_t\\}$ is to the autocorrelation function of the desired signal $\\{X_t\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. The variance of the measured signal is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[X_t + e_t] = \\text{Var}[X_t] + \\text{Var}[e_t] = \\sigma_X^2 + \\sigma_e^2 $$\n",
    "\n",
    "We have, for $k \\geq 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + e_t, X_{t - k} + e_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\sigma_X^2 \\rho_k $$\n",
    "\n",
    "where we have used independence to cancel out the covariance between $X_a$ and $e_b$ and the white noise property to cancel out $\\text{Cov}[e_t, e_{t - k}]$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_X^2 \\rho_k}{\\sigma_X^2 + \\sigma_e^2} = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.25**.  Suppose $Y_t = \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] $, where $\\beta_0, f_1, f_2, \\dots, f_k$ are constants and $A_1, A_2, \\dots, A_k, B_1, B_2, \\dots, B_k$ are independent random variables with zero means and variances $\\text{Var}[A_i] = \\text{Var}[B_i] = \\sigma_i^2$. Show that $\\{Y_i\\}$ is stationary and find its covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have mean\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y_t] &= \\text{E}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\beta_0 + \\sum_{i=1}^k \\left( \\text{E}[A_i] \\cos(2 \\pi f_i t)  + \\text{E}[B_i] \\sin(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\beta_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[Y_t] &= \\text{Var}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\left (\\text{Var}[A_i] \\cos^2(2 \\pi f_i t) + \\text{Var}[B_i] \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos^2(2 \\pi f_i t) + \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the trigonometric identity $\\cos^2 \\alpha + \\sin^2 \\alpha = 1$.\n",
    "\n",
    "The autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)], \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i (t - k)) + B_i \\sin(2 \\pi f_i (t - k))] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\sum_{j=1}^k \\text{Cov}[A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t), A_j \\cos(2 \\pi f_i (t - k)) + B_j \\sin(2 \\pi f_i (t - k))] \\\\\n",
    "&= \\sum_{i=1}^k \\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) \\text{Var}[A_i] + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\text{Var}[B_i]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where, since the $A_i$'s and $B_i$'s are all independent, we cancel out all covariance terms that do not repeat the same random variable on both sides.\n",
    "\n",
    "Continuing the calculation, and using the trigonometric identify $\\cos (\\alpha - \\beta) = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}]| \n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i t - 2 \\pi f_i (t-k)) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is free of $t$.  The autocovariance function is, more explicitly,\n",
    "\n",
    "$$ \\gamma_k = \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k) $$\n",
    "\n",
    "and we have shown that $\\{Y_i\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.26**.  Define the function $\\Gamma_{t, s} = \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2]$.  In geostatistics, $\\Gamma_{t, s}$ is called the **semivariogram**.\n",
    "\n",
    "**(a)** Show that for a stationary process $\\Gamma_{t, s} = \\gamma_0 - \\gamma_{|t - s|}$.\n",
    "\n",
    "**(b)** A process is said to be **intrinsically stationary** if $\\Gamma_{t, s}$ depends only on the time difference $|t - s|$.  Show that the random walk process is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** For a stationary process,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] = \\frac{1}{2} \\text{E}[Y_t^2 + Y_s^2 - 2 Y_t Y_s] \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{E}[Y_t^2] + \\text{E}[Y_s^2] - 2 \\text{E}[Y_t Y_s] \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{Var}[Y_t] + \\text{E}[Y_t]^2 + \\text{Var}[Y_s] + \\text{E}[Y_s]^2 - 2 \\left( \\text{Cov}[Y_t, Y_s] + \\text{E}[Y_t] E[Y_s] \\right) \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\gamma_0 + \\overline{Y}^2 + \\gamma_0 + \\overline{Y}^2 - 2 (\\gamma_{|t - s|} + \\overline{Y}^2) \\right) \\\\\n",
    "&= \\gamma_0 - \\gamma_{|t - s|} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  For the random walk, assuming without loss of generality $t \\leq s$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\left(\\sum_{j=t+1}^s e_j \\right)^2 \\right] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\sum_{j=t+1}^s e_j^2 + \\sum_i \\sum_{j, j \\neq i} e_i e_j\\right] \\\\\n",
    "&= \\frac{1}{2} \\sum_{j=t+1}^s \\text{E} [e_j^2] + \\sum_i \\sum_{j, j \\neq i} \\text{E}[e_i] \\text{E}[e_j] \\\\\n",
    "&= \\frac{1}{2} (s - t) \\sigma_e^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while, for $t < s$, we can do $\\Gamma_{t, s} = \\Gamma_{s, t} = \\frac{1}{2} (t - s) \\sigma_e^2$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\Gamma_{s, t} = \\frac{1}{2} |s - t| \\sigma_e^2 $$\n",
    "\n",
    "and so a random walk is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.27**.  For a fixed, positive integer $r$ and constant $\\phi$, consider the time series defined by $Y_t = e_t + \\phi e_{t - 1} + \\phi^2 e_{t - 2} + \\cdots + \\phi^r e_{t - r}$.\n",
    "\n",
    "**(a)** Show that this process is stationary for any value of $\\phi$.\n",
    "\n",
    "**(b)** Find the autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] \n",
    "= \\text{E}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right] \n",
    "= \\sum_{j=0}^r \\phi^j \\text{E}[e_{t-j}] \n",
    "= \\sum_{j=0}^r \\phi^j \\cdot 0\n",
    "= 0\n",
    "$$\n",
    "\n",
    "which is constant (and does not depend on $t$).\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] \n",
    "= \\text{Var}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right]\n",
    "= \\sum_{j=0}^r \\phi^{2j} \\text{Var}[e_{t - j}] \n",
    "= \\sigma_e^2 \\sum_{j=0}^r \\phi^{2j}\n",
    "$$\n",
    "\n",
    "where this is a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\begin{cases}\n",
    "(r + 1) \\sigma_e^2 &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\sigma_e^2 \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "Finally, the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=0}^r \\phi^i e_{t - i}, \\sum_{j=0}^r \\phi^j e_{t - k - j} \\right] \\\\\n",
    "&= \\sum_{i=0}^r \\sum_{j=0}^r \\phi^{i + j}\\text{Cov}[e_{t - i}, e_{t - k - j} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The only non-zero terms in the sum occur when the indexes of both variables are the same, which is to say, $i = k + j$, with $i, j \\in [0, r]$.  Assuming $k \\leq r$, we can then write the somewhat simpler formula,\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - k}] = \\sigma_e^2 \\sum_{i=0}^r \\phi^{2i - k} = \\frac{\\sigma_e^2}{\\phi^k} \\sum_{i = 0}^r \\phi^{2i}\n",
    "$$\n",
    "\n",
    "which is again a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "(r + 1) \\frac{\\sigma_e^2}{\\phi^k} &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\frac{\\sigma_e^2}{\\phi^k} \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "As this is not a function of time either, the process $\\{ Y_t \\}$ is stationary for any value of $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As we already computed the autocovariance, the autocorrelation is relatively straightforward,\n",
    "\n",
    "$$ \\rho_k = \\gamma_k / \\gamma_0 $$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "\\phi^{-k} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.28 (Random cosine wave extended)**.  Suppose that\n",
    "\n",
    "$$ Y_t = R \\cos (2 \\pi (f t + \\Phi)) \\quad \\text{for } t = 0, \\pm 1, \\pm 2, \\dots $$\n",
    "\n",
    "where $0 < f < \\frac{1}{2}$ is a fixed frequency and $R$ and $\\Phi$ are uncorrelated random variables and with $\\Phi$ uniformly distributed on the interval (0, 1).\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$ for all $t$.\n",
    "\n",
    "**(b)** Show that the process is stationary with $\\gamma_k = \\frac{1}{2} \\text{E}[R^2] \\cos (2 \\pi f k)$.\n",
    "\n",
    "Hint: Use the calculations leading up to Equation (2.3.4), on page 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
